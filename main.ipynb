{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15756f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 13:42:56.223604: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-03 13:42:56.225931: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-03 13:42:56.229090: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-03 13:42:56.235041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759513376.247607 3578329 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759513376.251377 3578329 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759513376.262695 3578329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759513376.262706 3578329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759513376.262708 3578329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759513376.262709 3578329 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-03 13:42:56.266324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pyreadstat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_plink\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
    "\n",
    "import tensorflow as tf\n",
    "#print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb281f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization parameters\n",
    "\n",
    "# Training CEDS scale\n",
    "# Choose the target data A = 19 predictors or B = 10 predictors\n",
    "#CESD_type = 'B' # 'A' or 'B'\n",
    "\n",
    "# Choose the target data Wave: Wave I or Wave IV\n",
    "#test_wave = 'Wave I' # 'Wave I' or 'Wave IV'\n",
    "\n",
    "# Model single (only one neural network is trained) \n",
    "# Model multiple (several differents neural networks are trained and the best one is chosen)\n",
    "# DO NOT FORGET IF YOU ARE USING MULTIPLE TO CHECK IF THERE IS A FOLDER CALLED hyperparameters-log\n",
    "# IF THERE IS THE MODEL WILL NOT TRAIN PROPERLY\n",
    "#model_type = 'single'\n",
    "\n",
    "# Include Genetic data? (boolean)\n",
    "#include_genetic_data = True\n",
    "\n",
    "# Account for ancestry\n",
    "#control_ancestry = True # Error when controling for ancestry I dont know why\n",
    "\n",
    "# Account for race\n",
    "#control_race = False\n",
    "\n",
    "# Choose sex\n",
    "#sex_gender = None # 'Male' or 'Female' or None\n",
    "\n",
    "info_dict = {\n",
    "    \"CESD_type\": \"A\",  # 'A' (19 predictors) or 'B' (10 predictors)\n",
    "    \"test_wave\": 'Wave I',  # 'Wave I' or 'Wave IV' for the first wave\n",
    "    \"model_type\": \"single\",  # 'single' (one NN) or 'multiple' (best of several NNs)\n",
    "    \"include_genetic_data\": False,  # Include genetic data or not\n",
    "    \"control_ancestry\": True,  # Account for ancestry (error noted)\n",
    "    \"control_race\": False,  # Account for race\n",
    "    \"sex_gender\": None,  # 'Male', 'Female', or None\n",
    "    'include_environmental_data': True, # Include environmental data or not\n",
    "    'include_depressive_symptoms': False, # Include depressive symptos\n",
    "    'clinically_depressed': None # adolescence or adulthood or None (also need to put test_wave = Wave IV)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f05c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_metrics(y_true, y_prob, n_bootstraps=1000, ci=95):\n",
    "    \"\"\"Computes percentile bootstrap confidence intervals for AUC.\"\"\"\n",
    "    aucs = []\n",
    "    accuracies = []\n",
    "\n",
    "    # For a sigmoid output, threshold at 0.5 to get binary predictions\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    # Generate bootstrap samples\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample indices with replacement\n",
    "        indices = resample(range(len(y_true)), replace=True)\n",
    "        y_true_boot = np.array(y_true)[indices]\n",
    "        y_prob_boot = np.array(y_prob)[indices]\n",
    "        y_pred_boot = np.array(y_pred)[indices]\n",
    "        \n",
    "        # Compute AUC for this bootstrap sample\n",
    "        aucs.append(roc_auc_score(y_true_boot, y_prob_boot))\n",
    "    \n",
    "        # Compute Accuracy for this bootstrap sample\n",
    "        accuracies.append(accuracy_score(y_true_boot, y_pred_boot))\n",
    "\n",
    "    # Sort the AUC values\n",
    "    aucs = np.sort(aucs)\n",
    "    # Sort the accuracy values\n",
    "    accuracies = np.sort(accuracies)\n",
    "    \n",
    "    # Compute percentiles for CI\n",
    "    lower_percentile = (100 - ci) / 2\n",
    "    upper_percentile = 100 - lower_percentile\n",
    "    \n",
    "    lower_bound_auc = np.percentile(aucs, lower_percentile)\n",
    "    upper_bound_auc = np.percentile(aucs, upper_percentile)\n",
    "\n",
    "    lower_bound_acc = np.percentile(accuracies, lower_percentile)\n",
    "    upper_bound_acc = np.percentile(accuracies, upper_percentile)\n",
    "    \n",
    "    return aucs, np.mean(aucs), lower_bound_auc, upper_bound_auc, np.mean(accuracies), lower_bound_acc, upper_bound_acc\n",
    "\n",
    "def plot_confusion_matrix_and_roc(y_true, model, X_test, title,info_dict, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix on the left and ROC curve with AUC on the right.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Actual labels.\n",
    "    - model: Trained model to generate predicted probabilities for ROC curve.\n",
    "    - X_test: Test features, required to calculate predicted probabilities.\n",
    "    - title: Title for the plot.\n",
    "    - threshold: The threshold to apply on predicted probabilities for classification (default is 0.5).\n",
    "    - info_dict: Information about the data, use to plot different colors\n",
    "    \"\"\"\n",
    "    # 1. Predict probabilities for the test set (needed for ROC curve and prediction)\n",
    "    y_prob = model.predict(X_test)\n",
    "    \n",
    "    # 2. For a sigmoid output, threshold at 0.5 to get binary predictions\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # 3. Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # 4. Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # 5. Compute ROC and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # 6. Create subplots: left for confusion matrix, right for ROC curve\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))  # Create 1 row, 2 columns layout\n",
    "\n",
    "    # Colors\n",
    "    \n",
    "    if info_dict['sex_gender'] == 'Female':\n",
    "        cmaps = 'Reds'\n",
    "        colors = 'r' \n",
    "    elif info_dict['sex_gender'] == 'Male':\n",
    "        cmaps = 'Blues'\n",
    "        colors = 'b'\n",
    "    else: \n",
    "        cmaps = 'gray'\n",
    "        colors = 'k'\n",
    "        \n",
    "    # Compute AUC with bootstrap confidence intervals\n",
    "    aucs, auc_mean, auc_lower, auc_upper, acc_mean, acc_lower, acc_upper = bootstrap_metrics(y_true, y_prob)\n",
    "    print(f\"AUC: {auc_mean:.3f} (CI: {auc_lower:.3f} - {auc_upper:.3f})\")\n",
    "    print(f\"accuracy: {acc_mean:.3f} (CI: {acc_lower:.3f} - {acc_upper:.3f})\")    \n",
    "        \n",
    "    # Left plot: Confusion Matrix\n",
    "    ax1 = axes[0] \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmaps, annot_kws={'size': 16}, \n",
    "                xticklabels=['Does not have\\nsymptoms of depression (0)', \n",
    "                             'Has symptoms of\\n depression (1)'], \n",
    "                yticklabels=['Does not have\\nsymptoms of depression (0)', \n",
    "                             'Has symptoms of\\n depression (1)'], \n",
    "                cbar_kws={'label': 'Count'}, linewidths=1, linecolor='gray', \n",
    "                vmin=0, vmax=np.max(cm), ax=ax1)\n",
    "    ax1.set_xlabel('Predicted', fontsize=12)\n",
    "    ax1.set_ylabel('Actual', fontsize=12)\n",
    "    \n",
    "    # Add metrics text next to the confusion matrix\n",
    "    metrics_text = f\"Accuracy: {accuracy:.2f}\\nRecall: {recall:.2f}\\nF1 Score: {f1:.2f}\"\n",
    "    ax1.text(.8, 1, metrics_text, fontsize=12, ha='left', va='center', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Right plot: ROC Curve\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(fpr, tpr, color=colors, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    ax2.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    ax2.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax2.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax2.set_title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "    # Set the plot title\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return aucs, auc_mean, auc_lower, auc_upper, acc_mean, acc_lower, acc_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb81782",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To list all folders and files in directory\n",
    "\n",
    "def find_files_folders(directory_path):\n",
    "    files_and_folders = os.listdir(directory_path)\n",
    "    print(files_and_folders)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a6342",
   "metadata": {},
   "source": [
    "# Some of the test group in WAVE I are not present in wave IV (I can change to make only the training in wave I and test in WAVE IV to compare)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98447a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "CUDA is not available. TensorFlow is not using the GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 13:42:58.131687: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is avaliable\n",
    "\n",
    "print(tf.__version__)  # Print the TensorFlow version\n",
    "\n",
    "# Check CUDA and cuDNN versions used by TensorFlow\n",
    "# Check if TensorFlow can access CUDA-enabled GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"CUDA is available! TensorFlow is using the GPU.\")\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    from tensorflow.python.platform import build_info as tf_build_info\n",
    "    print(\"CUDA Version: \", tf_build_info.cuda_version)\n",
    "    print(\"cuDNN Version: \", tf_build_info.cudnn_version)\n",
    "else:\n",
    "    print(\"CUDA is not available. TensorFlow is not using the GPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de4da2",
   "metadata": {},
   "source": [
    "# Wave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b5a22",
   "metadata": {},
   "source": [
    "## Environmental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f39711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20745, 83)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .xpt file\n",
    "df1, meta1 = pyreadstat.read_xport('REPLACE_WITH_YOUR_WAVE_I_FILE_LOCATION') # file location\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data types\n",
    "dtypes = df1.dtypes\n",
    "\n",
    "# Convert to DataFrame\n",
    "dtypes_df = dtypes.reset_index()\n",
    "dtypes_df.columns = ['Column Name', 'Data Type']\n",
    "\n",
    "# Save to CSV\n",
    "dtypes_df.to_csv('data_types.csv', index=False)\n",
    "\n",
    "# count\n",
    "dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48aee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for better explanation\n",
    "\n",
    "df1.rename(columns={'BIO_SEX_numeric': 'SEX', # 1 males 2 females\n",
    "                  'race_recoded_numeric':'RACE',\n",
    "                  'PA55':'INCOME',\n",
    "                  'PA56_numeric':'ENOUGH MONEY FOR BILLS',\n",
    "                  'PA59_numeric':'MEDICAL CARE FOR FAMILY',\n",
    "                  'H1NB5_numeric':'FEEL SAFE IN NBORHOOD?',\n",
    "                  'H1NB6_numeric':'HOW HAPPY LIVING IN NBORHOOD',\n",
    "                  'H1GH1_numeric':'GENERAL HEALTH',\n",
    "                  'H1PL1_numeric':'DIFFICULTY USING HANDS/FT/LIMBS',\n",
    "                  'H1GH2_numeric':'FREQ-HEADACHES',\n",
    "                  'H1GH3_numeric':'FREQ-FEELING HOT',\n",
    "                  'H1GH4_numeric':'FREQ-STOMACHACHE',\n",
    "                  'H1GH5_numeric':'FREQ-COLD SWEATS',\n",
    "                   'H1GH6_numeric':'FREQ-FEELING PHYSICALLY WEAK', # 0 never / 1 few times / 2 once a week / 3 almost every day / 4 every day\n",
    "                   'H1GH7_numeric':'FREQ-SORE THROAT/COUGH',\n",
    "                   'H1GH8_numeric':'FREQ-VERY TIRED FOR NO REASON',\n",
    "                   'H1GH9_numeric':'FREQ-PAINFUL/OFTEN URINATION',\n",
    "                   'H1GH10_numeric':'FREQ-FEELING VERY SICK',\n",
    "                   'H1GH11_numeric':'FREQ-WAKE UP FEELING TIRED',\n",
    "                   'H1GH12_numeric':'FREQ-SKIN PROBLEMS',\n",
    "                   'H1GH13_numeric':'DIZZINESS',\n",
    "                   'H1GH14_numeric':'CHEST PAINS',\n",
    "                   'H1GH15_numeric':'FREQ-MUSCLE/JOINT ACHES/PAINS',\n",
    "                   'H1GH16_numeric_recode':'FREQ-MENSTRUAL CRAMPS',\n",
    "                   'H1GH17_numeric':'FREQ-POOR APPETITE',\n",
    "                   'H1GH18_numeric':'FREQ-INSOMNIA',\n",
    "                   'H1GH19_numeric':'FREQ-TROUBLE RELAXING',\n",
    "                   'H1GH22_numeric':'FREQ-FEARFULNESS',\n",
    "                   'H1GH26_numeric':'NEEDED BUT NOT GET MEDICAL CARE',\n",
    "                   'H1GH27I_numeric_recode':'WHY-COULD NOT PAY',\n",
    "                   'H1GH28_numeric':'WEIGHT IMAGE',\n",
    "                   'H1GH29_numeric':'TO GAIN/LOSE/MAINTAIN WEIGHT',\n",
    "                   'H1HS3_numeric':'PSYCHOLOGICAL COUNSELING',\n",
    "                   'H1GH48_numeric':'HEALTH CAUSE SCHOOL ABSENCE',\n",
    "                   'H1GH49_numeric':'HEALTH CAUSE SOCIAL ABSENCE',\n",
    "                   'H1ED15_numeric':'TROUBLE-GETTING ALONG TEACHERS',\n",
    "                   'H1ED16_numeric':'TROUBLE-PAYING ATTENTION SCHOOL',\n",
    "                   'H1ED17_numeric':'TROUBLE-GETTING HOMEWORK DONE',\n",
    "                   'H1ED18_numeric':'TROUBLE-WITH OTHER STUDENTS',\n",
    "                   'H1ED19_numeric':'FEEL CLOSE TO PEOPLE AT SCHOOL',\n",
    "                   'H1ED20_numeric':'FEEL PART OF YOUR SCHOOL',\n",
    "                   'H1ED21_numeric':'STUDENTS AT SCHOOL PREJUDICED',   # 1 strong agree / 2 agree / 3 neutral / 4 disagree / 5 strong disagree\n",
    "                   'H1ED22_numeric':'HAPPY AT YOUR SCHOOL', # 1 strong agree / 2 agree / 3 neutral / 4 disagree / 5 strong disagree\n",
    "                   'H1ED23_numeric':'TEACHERS TREAT STUDENTS FAIRLY',\n",
    "                   'H1ED24_numeric':'FEEL SAFE IN YOUR SCHOOL',\n",
    "                   'H1DA7_numeric':'HANG OUT WITH FRIENDS',\n",
    "                   'H1WP9_numeric':'CLOSE TO MOM',\n",
    "                   'H1WP10_numeric':'MOM-HOW MUCH DOES SHE CARE',\n",
    "                   'H1WP13_numeric':'CLOSE TO DAD',\n",
    "                   'H1WP14_numeric':'DAD-HOW MUCH DOES HE CARE',\n",
    "                   'H1PF1_numeric':'MOM-WARM AND LOVING',\n",
    "                   'H1PF4_numeric':'MOM-GOOD COMMUNICATION',\n",
    "                   'H1PF5_numeric':'MOM-GOOD RELATIONSHIP',\n",
    "                   'H1PF24_numeric':'DAD-GOOD COMMUNICATION',\n",
    "                   'H1PF25_numeric':'DAD-GOOD RELATIONSHIP',\n",
    "                   'H1PR1_numeric':'ADULTS CARE ABOUT',\n",
    "                   'H1PR2_numeric':'TEACHERS CARE ABOUT YOU',\n",
    "                   'H1PR3_numeric':'PARENTS CARE ABOUT YOU',\n",
    "                   'H1PR4_numeric':'FRIENDS CARE ABOUT YOU',\n",
    "                   'H1PR5_numeric':'FAMILY UNDERSTAND YOU',\n",
    "                   'H1PR6_numeric':'WANT TO LEAVE HOME',\n",
    "                   'H1PR7_numeric':'FAMIYL HAS FUN TOGETHER',\n",
    "                   'H1PR8_numeric':'FAMILY PAYS ATTENTION TO YOU',\n",
    "                   'H1PF33_numeric':'LIKE SELF AS ARE',\n",
    "                   'H1PF34_numeric':'DO EVERYTHING JUST RIGHT',\n",
    "                   'H1PF35_numeric':'FEEL SOCIALLY ACCEPTED',\n",
    "                   'H1PF36_numeric':'FEEL LOVED AND WANTED',\n",
    "                   'H1CO10F_numeric_recode':'FORCED SEX',\n",
    "                   'H1NR3_numeric':'SEX FOR DRUGS OR MONEY',\n",
    "                   'H1FV1_numeric':'SAW SHOOTING/STABBING OF PERSON',\n",
    "                   'H1FV2_numeric':'HAD KNIFE/GUN PULLED ON YOU',\n",
    "                   'H1FV3_numeric':'SOMEONE SHOT YOU',\n",
    "                   'H1FV4_numeric':'SOMEONE STABBED YOU',\n",
    "                   'H1FV5_numeric':'GOT INTO A PHYSICAL FIGHT',\n",
    "                   'H1FV6_numeric':'WERE JUMPED',\n",
    "                   'H1FP21_numeric_recode':'PREG1 PREGNANCY OUTCOME',\n",
    "                   'H1SU4_numeric':'PAST YR-FRIENDS ATTEMPT SUICIDE',\n",
    "                   'H1SU5_numeric_recode':'PAST YR-FRIENDS SUCCEED',\n",
    "                   'H1SU6_numeric':'PAST YR-FAMILY ATTEMPT SUICIDE',\n",
    "                   'H1SU7_numeric_recode':'PAST YR-FAMILY SUCCEED'\n",
    "                  }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb20d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove depressive symptoms if needed\n",
    "\n",
    "if not info_dict.get(\"include_depressive_symptoms\", True):\n",
    "    columns_to_remove_symp = [\n",
    "        \"FREQ-POOR APPETITE\",             # Question 5\n",
    "        \"FREQ-TROUBLE RELAXING\",          # Question 7\n",
    "        \"FREQ-FEARFULNESS\",\n",
    "        \"FREQ-VERY TIRED FOR NO REASON\"   # Question 4\n",
    "    ]\n",
    "    df1 = df1.drop(columns=[col for col in columns_to_remove_symp if col in df1.columns])\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd1c1e9",
   "metadata": {},
   "source": [
    "## Genetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Skipped for now\n",
    "\n",
    "# Genetic information path\n",
    "#path = '/pdfs/rche/addhealth/AH_gwas/'\n",
    "#dir_list = os.listdir(path)\n",
    "#print(\"Files and directories in '\", path, \"' :\")\n",
    "# prints all files\n",
    "#print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene4, meta_gene4 = pyreadstat.read_xport(\"REPLACE_WITH_PGS_LOCATION\")\n",
    "\n",
    "print('Ancestry')\n",
    "print(gene4['PSANCEST'].value_counts())\n",
    "\n",
    "print('Race')\n",
    "print(gene4['PSRACE'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access metadata\n",
    "print(\"Variable Names and Labels:\")\n",
    "for var_name, label in zip(meta_gene4.column_names, meta_gene4.column_labels):\n",
    "    print(f\"{var_name}: {label if label else 'No label'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting value counts\n",
    "ancest_counts = gene4['PSANCEST'].value_counts()\n",
    "race_counts = gene4['PSRACE'].value_counts()\n",
    "\n",
    "# Creating dictionaries to store value counts for conversion\n",
    "ancest_dict = ancest_counts.to_dict()\n",
    "race_dict = race_counts.to_dict()\n",
    "\n",
    "# Remove the last entry from race_dict by slicing\n",
    "race_dict = dict(list(race_dict.items())[:-1])\n",
    "\n",
    "# Remove the last entry from race_counts\n",
    "race_counts = race_counts.iloc[:-1]\n",
    "\n",
    "# Printing the dictionaries\n",
    "print(\"Ancestor Counts:\", ancest_dict)\n",
    "print(\"Race Counts:\", race_dict)\n",
    "\n",
    "# Plotting the pie chart for PSANCEST with adjusted label positions\n",
    "plt.figure(figsize=(8, 6))\n",
    "wedges, texts, autotexts = plt.pie(ancest_counts, labels=ancest_counts.index, autopct='%1.1f%%', startangle=90,\n",
    "                                   wedgeprops=dict(edgecolor='black', linewidth=1.5), textprops=dict(color='black', fontsize=10),\n",
    "                                   labeldistance=1.1)\n",
    "\n",
    "plt.title('Ancestry',fontsize=16)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()\n",
    "\n",
    "# Plotting the pie chart for PSRACE with a different labeldistance\n",
    "plt.figure(figsize=(8, 6))\n",
    "wedges, texts, autotexts = plt.pie(race_counts, labels=race_counts.index, autopct='%1.1f%%', startangle=90,\n",
    "                                   wedgeprops=dict(edgecolor='black', linewidth=1.5), textprops=dict(color='black', fontsize=12),\n",
    "                                   labeldistance=1.1)\n",
    "\n",
    "# Manually adjusting the label positions for overlap\n",
    "for text in texts:\n",
    "    text.set_fontsize(10)\n",
    "    text.set_color('black')\n",
    "\n",
    "# Adjusting label positions\n",
    "texts[3].set_position((.17, 1.05))  # Move \"Asian\" further out\n",
    "texts[4].set_position((-0.15, 1.06))  # Move \"Nat. Am.\" further out\n",
    "\n",
    "# Adjusting percentage positions\n",
    "autotexts[3].set_position((.11, 0.6))  # Move percentage for \"Asian\" further out\n",
    "autotexts[4].set_position((-.10, 0.6))  # Move percentage for \"Nat. Am.\" further out\n",
    "\n",
    "plt.title('Race',fontsize=16)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ebdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping unused collumns\n",
    "imputed_gene4 = gene4\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Account for ancestry\n",
    "if info_dict['control_ancestry']:\n",
    "    imputed_gene4 = imputed_gene4[imputed_gene4['PSANCEST'] == 'European ancestry']\n",
    "    imputed_gene4 = imputed_gene4.drop(columns=['PSANCEST'])\n",
    "    imputed_gene4 = imputed_gene4.reset_index(drop=True)\n",
    "else:\n",
    "    # Encoding the categorical column ancestry\n",
    "    imputed_gene4['PSANCEST'] = label_encoder.fit_transform(imputed_gene4['PSANCEST'])\n",
    "    \n",
    "# Account for race\n",
    "if info_dict['control_race']:\n",
    "    imputed_gene4 = imputed_gene4[imputed_gene4['PSRACE'] == 'NHW']\n",
    "    imputed_gene4 = imputed_gene4.drop(columns=['PSRACE'])\n",
    "    imputed_gene4 = imputed_gene4.reset_index(drop=True)\n",
    "else:\n",
    "    # Encoding the categorical column race\n",
    "    imputed_gene4['PSRACE'] = label_encoder.fit_transform(imputed_gene4['PSRACE'])\n",
    "\n",
    "imputed_gene4.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c15021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# website: https://addhealth-navigator.cpc.unc.edu/data/example.org/a6259610-271b-40f2-aca5-d4d22251a875/example.org/e7c8a7d8-df14-4e49-84c8-d2ac77256ac5/example.org/d79b1b15-2942-469e-87d1-fe045aa91757\n",
    "dictw = {\n",
    "    \"FID\": \"FID\",\n",
    "    \"AID\": \"AID\",\n",
    "    \"PSCADSTD\": \"CARDIoGRAM Coronary Artery Disease\",\n",
    "    \"PSBMI1\":\"GIANT BMI, 2018\",\n",
    "    \"PSADSTD\": \"Alzheimer's Diease, 2019\",\n",
    "    \"PSEDU1\":\"Educational Attainment, 2018\",\n",
    "    \"PSAUTISM\":\"Autism Spectrum Disorder, 2017\",\n",
    "    \"PSEVRSK1\":\"Ever Regular Smoker, 2019\",\n",
    "    \"PSADEA\":\"CTG Alcohol Dependence, 2019\",\n",
    "    \"PSMENARC\":\"Menarche, 2014\",\n",
    "    \"PSMENO\":\"Menopause, 2015\",\n",
    "    \"PSPTSDAL\":\"PTSD, 2018\", # sub: all (x) - EA - AA\n",
    "    \"PSADHD2\":\"Attention-deficit/hyperactivity disorder, 2017\",\n",
    "    \"PSBIPOL\":\"Bipolar, 2011\",\n",
    "    \"PSSCZSTD\":\"Schizophrenia, 2011\",\n",
    "    \"PSMDD1\":\"Major Depressive Disorder, 2019\",\n",
    "    \"PSPOSAF1\":\"Wellbeing - Positive Affection, 2019\", # sub: Dep - LifeSat - Neuroticism - PosAff (x)\n",
    "    \"PSEXTRAV\":\"Extraversion, 2015\",\n",
    "    \"PSNEURO1\":\"Neuroticism, 2019\",\n",
    "    \"PSANSTD\":\"Anorexia Nervosa, 2019\", # 90% sure\n",
    "    \"PSSOCSTD\":\"Social Isolation, 2018\", # PGC_Social_2018_PubClub PGC_Social_2018(x)\n",
    "    \"PSLONELY\":\"Lonely, 2018\",\n",
    "    \"PSINSOMN\":\"Insomnia, 2019\", # sub: nosub(x) - Dozing - Morning - Napping - Snoring - GetUp - SleepDur\n",
    "}\n",
    "\n",
    "# Step 1: Filter columns\n",
    "imputed_gene4 = imputed_gene4[[col for col in imputed_gene4.columns if col in dictw]]\n",
    "\n",
    "# Step 2: Rename columns\n",
    "imputed_gene4.rename(columns=dictw, inplace=True)\n",
    "\n",
    "# Output the resulting DataFrame\n",
    "imputed_gene4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457fa1f",
   "metadata": {},
   "source": [
    "# Getting the depression score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b966ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CESD_W1A is using 19 questions\n",
    "# CESD_W1B is using 10 questions\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a1cd1",
   "metadata": {},
   "source": [
    "A score of 10 or greater indicates symptoms of depression; \n",
    "\n",
    "16 or higher indicates severe symptoms of depression.\n",
    "\n",
    "Obs: we are using the incomplete version of the test, so we need to correct for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcc0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "def plot_histogram_with_stats(data, column_name, line_at=16):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the specified column and calculates basic statistics, \n",
    "    including mean, standard deviation, skewness, and kurtosis. A vertical dashed \n",
    "    line is drawn at the specified location.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing the data.\n",
    "    column_name (str): The name of the column to be analyzed and plotted.\n",
    "    line_at (float, optional): The x-position for the vertical dashed line. Default is 16.\n",
    "\n",
    "    Returns:\n",
    "    None: The function displays the histogram plot and statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot histogram of the specified column\n",
    "    data[column_name].hist(bins=20, edgecolor='black')\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean_score = data[column_name].mean()\n",
    "    std_dev_score = data[column_name].std()\n",
    "\n",
    "    # Skewness calculation\n",
    "    skewness = (np.sum((data[column_name] - mean_score) ** 3) / \n",
    "                len(data[column_name])) / (std_dev_score ** 3)\n",
    "\n",
    "    # Kurtosis\n",
    "    kurt = kurtosis(data[column_name])\n",
    "\n",
    "    # Add a vertical red line at the specified location\n",
    "    plt.axvline(x=line_at, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.xlabel(f'{column_name} Score', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "    # Annotate the plot with mean, standard deviation, skewness, and kurtosis\n",
    "    plt.gca().text(0.95, 0.95, \n",
    "                   f'Mean: {mean_score:.2f}\\nStd Dev: {std_dev_score:.2f}\\nSkewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\\nSize: {len(data)}', \n",
    "                   horizontalalignment='right',\n",
    "                   verticalalignment='top',\n",
    "                   transform=plt.gca().transAxes,\n",
    "                   bbox=dict(facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb1153",
   "metadata": {},
   "source": [
    "## Depression Category W1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 has symptoms of depression / 0 does not has symptoms of depression\n",
    "\n",
    "df1['category_W1A'] = df1['CESD_W1A'].apply(lambda x: 0 if x < 16 else 1)\n",
    "df1['category_W1A'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_with_stats(df1, 'CESD_W1A',line_at=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70cb70",
   "metadata": {},
   "source": [
    "## Depression Category W1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['category_W1B'] = df1['CESD_W1A'].apply(lambda x: 0 if x < 8 else 1)\n",
    "df1['category_W1B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9274f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_with_stats(df1, 'CESD_W1B',line_at=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping ID column and the two scores A and B\n",
    "df1.drop(columns=['CESD_W1A','CESD_W1B'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873421ea",
   "metadata": {},
   "source": [
    "# Wave IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4, meta4 = pyreadstat.read_xport('REPLACE_WITH_WAVE_IV_LOCATION')\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026de32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinaically Depressed\n",
    "\n",
    "# https://addhealth.cpc.unc.edu/documentation/codebook-explorer/#/variable_collection/1077\n",
    "# https://addhealth.cpc.unc.edu/documentation/codebook-explorer/#/variable_collection/1076\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Filter out the entries 96, 97, and 98 from df4['H4ID6H']\n",
    "clin_depre = df4[~df4['H4ID6H'].isin([96, 97, 98])]['H4ID6H']\n",
    "\n",
    "# Define the age ranges\n",
    "childhood_range = (0, 10)  # Childhood range\n",
    "adolescence_range = (10, 20)\n",
    "adulthood_range = (20, 100)  # Assuming age goes up to 100 for the adulthood range\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the histogram\n",
    "n, bins, patches = plt.hist(clin_depre, bins=30, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels for the plot\n",
    "plt.xlabel('Age of Depression (H4ID6H)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('')\n",
    "\n",
    "# Highlight the childhood area (0-12)\n",
    "childhood_bin_start = (bins >= childhood_range[0]).argmax()\n",
    "childhood_bin_end = (bins <= childhood_range[1]).argmin()\n",
    "for i in range(childhood_bin_start, childhood_bin_end):\n",
    "    patches[i].set_facecolor('green')\n",
    "\n",
    "# Highlight the adolescence area (12-18)\n",
    "adolescence_bin_start = (bins >= adolescence_range[0]).argmax()\n",
    "adolescence_bin_end = (bins <= adolescence_range[1]).argmin()\n",
    "for i in range(adolescence_bin_start, adolescence_bin_end):\n",
    "    patches[i].set_facecolor('orange')\n",
    "\n",
    "# Highlight the adulthood area (18-100)\n",
    "adulthood_bin_start = (bins >= adulthood_range[0]).argmax()\n",
    "adulthood_bin_end = len(bins) - 1  # Ensure it goes to the last bin\n",
    "for i in range(adulthood_bin_start, adulthood_bin_end):\n",
    "    patches[i].set_facecolor('lightblue')\n",
    "\n",
    "# Calculate percentages of each area\n",
    "total_count = n.sum()\n",
    "\n",
    "# Calculate childhood count\n",
    "childhood_count = n[childhood_bin_start:childhood_bin_end].sum()\n",
    "\n",
    "# Calculate adolescence count\n",
    "adolescence_count = n[adolescence_bin_start:adolescence_bin_end].sum()\n",
    "\n",
    "# Calculate adulthood count\n",
    "adulthood_count = n[adulthood_bin_start:adulthood_bin_end].sum()\n",
    "\n",
    "# Calculate percentages\n",
    "childhood_percentage = (childhood_count / total_count) * 100\n",
    "adolescence_percentage = (adolescence_count / total_count) * 100\n",
    "adulthood_percentage = (adulthood_count / total_count) * 100\n",
    "\n",
    "# Create custom legend entries with the correct colors\n",
    "childhood_legend = Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label=f'Childhood: {childhood_percentage:.1f}% ({childhood_range[0]}-{childhood_range[1]-1})')\n",
    "adolescence_legend = Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label=f'Adolescence: {adolescence_percentage:.1f}% ({adolescence_range[0]}-{adolescence_range[1]-1})')\n",
    "adulthood_legend = Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', markersize=10, label=f'Adulthood: {adulthood_percentage:.1f}% ({adulthood_range[0]}+)')\n",
    "\n",
    "# Add the legend to the plot\n",
    "plt.legend(handles=[childhood_legend, adolescence_legend, adulthood_legend], loc='upper left', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_depre = (~df4['H4ID6H'].isin([96, 97, 98])) & (df4['H4ID6H'] >= adulthood_range[0])\n",
    "adolescence_depre =  (~df4['H4ID6H'].isin([96, 97, 98])) & \\\n",
    "                        (df4['H4ID6H'] >= adolescence_range[0]) & \\\n",
    "                        (df4['H4ID6H'] <= adolescence_range[1])\n",
    "\n",
    "# Create the new column 'adulthood_depression' based on the condition\n",
    "df4['adulthood_depression'] = np.where(adult_depre, 1, 0)\n",
    "df4['adolescence_depression'] = np.where(adolescence_depre, 1, 0)\n",
    "\n",
    "print('adolescence depression:',len(adolescence_depre == 1))\n",
    "print('adulthood depression:',len(adult_depre == 1))\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b302acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CESD for WAVE IV\n",
    "CESD4 = [\"H4MH18\",  # You were bothered by things that usually don't bother you.  \n",
    "        \"H4MH19\",  #  (During the past seven days:) You could not shake off the blues, even with help from your family and your friends.  \n",
    "        \"H4MH20\",  #  (During the past seven days:) You felt you were just as good as other people. REVERSE\n",
    "        \"H4MH21\",  # (During the past seven days:) You had trouble keeping your mind on what you were doing. \n",
    "        \"H4MH22\",  # (During the past seven days:) You felt depressed.\n",
    "        \"H4MH23\",  # (During the past seven days:) You felt that you were too tired to do things.  \n",
    "        \"H4MH24\",  # (During the past seven days:) You felt happy.  REVERSE\n",
    "        \"H4MH25\", # (During the past seven days:) You enjoyed life. REVERSE\n",
    "        \"H4MH26\", # (During the past seven days:) You felt sad.  \n",
    "        \"H4MH27\"] # (During the past seven days:) You felt that people disliked you, during the past seven days.  \n",
    "\n",
    "df4[CESD4].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03203ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 6 and 8 with NaN in the specified columns\n",
    "df4[CESD4] = df4[CESD4].replace([6, 8], np.nan)\n",
    "\n",
    "# Calculate the percentage of NaN values for each column\n",
    "nan_percentage = (df4[CESD4].isna().sum())/(df4[CESD4].sum())*100\n",
    "\n",
    "# Plotting the percentage of NaN values\n",
    "plt.figure(figsize=(10, 6))\n",
    "nan_percentage.sort_values().plot(kind='barh')\n",
    "plt.title('Percentage of NaN Values in Selected Columns')\n",
    "plt.xlabel('Percentage of NaN (%)')\n",
    "plt.ylabel('Columns')\n",
    "plt.axvline(x=0, color='grey', linestyle='--')  # optional: add a line at x=0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reversed coded: H4MH25,H4MH24,H4MH20\n",
    "reversed_coded_4 = ['H4MH25','H4MH24','H4MH20']\n",
    "\n",
    "# Apply the transformation (3 -> 0, 2 -> 1, 1 -> 2, 0 -> 3) to each of the reversed coded columns\n",
    "for col in reversed_coded_4:\n",
    "    df4[col] = df4[col].replace({3: 0, 2: 1, 1: 2, 0: 3})\n",
    "\n",
    "df4[CESD4].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b241ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the columns in CESD4 row-wise and store the result in a new column CESD_W4B\n",
    "df4['CESD_W4B'] = df4[CESD4].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd038dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_with_stats(df4, 'CESD_W4B',line_at=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAVE IV only has W4B (10 questions)\n",
    "df4['category_W4B'] = df4['CESD_W4B'].apply(lambda x: 0 if x < 10 else 1)\n",
    "df4['category_W4B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Wave I shape: ',df1.shape, '(only the 80 predictors + the targets)')\n",
    "print('Wave IV shape: ',df4.shape)\n",
    "print('Genetic files shape:',gene4.shape)\n",
    "\n",
    "count = df4['AID'].isin(df1['AID']).sum()\n",
    "count_gen = gene4['AID'].isin(df1['AID']).sum()\n",
    "print('Number of participants in both waves: ',count)\n",
    "print('Number of participants with genetic information: ',count_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245783e",
   "metadata": {},
   "source": [
    "# Final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4abb83",
   "metadata": {},
   "source": [
    "If include_genetic_data is False, you should not include gene4 in the final dataframe.\n",
    "\n",
    "If test_wave is 'Wave I', you should exclude df4 from the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sets of AID values from each dataframe\n",
    "#aid_df1 = set(df1['AID'])\n",
    "#aid_df4 = set(df4['AID'])\n",
    "#aid_gene4 = set(imputed_gene4['AID']) # getting the inputed version of gene4 (only the pre selected poly)\n",
    "\n",
    "# Find the intersection of the three sets\n",
    "#common_aids = aid_df1 & aid_df4 & aid_gene4\n",
    "\n",
    "# Convert the result to a sorted list\n",
    "#common_aids = sorted(common_aids)\n",
    "\n",
    "# Print the list of AIDs\n",
    "#print(common_aids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30eaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names for each dataframe\n",
    "#columns_df1 = df1.columns.tolist()\n",
    "#columns_df4 = df4.columns.tolist()\n",
    "#columns_gene4 = imputed_gene4.columns.tolist() # imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44f8de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merging the DataFrames using outer join on AID\n",
    "#df = pd.merge(df1, df4, on='AID', how='outer') \n",
    "#df = pd.merge(df, imputed_gene4, on='AID', how='outer') # imputed\n",
    "#Outer: Ensures all rows from the three dataframes are included, with NaN for missing data.\n",
    "\n",
    "\n",
    "# Droping individuals that do not have information in Wave IV or genetic\n",
    "# I could not just removing NaN because there are NaN in the files\n",
    "# Filter rows to keep only those with AID in common_aids\n",
    "#df = df[df['AID'].isin(common_aids)]\n",
    "\n",
    "# Reset the index for cleanliness\n",
    "#df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Result\n",
    "#print(df.shape)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the conditions in info_dict\n",
    "#include_genetic_data = info_dict['include_genetic_data']\n",
    "#test_wave = info_dict['test_wave']\n",
    "\n",
    "# Get the sets of AID values from each dataframe, conditional on the settings\n",
    "aid_df1 = set(df1['AID'])  # This is always included\n",
    "if info_dict['test_wave'] == 'Wave I':\n",
    "    # If the test_wave is 'Wave I', don't include df4\n",
    "    aid_df4 = set()  # Empty set, df4 is not used\n",
    "else:\n",
    "    # If the test_wave is 'Wave IV', include df4\n",
    "    aid_df4 = set(df4['AID'])\n",
    "\n",
    "if info_dict['include_genetic_data']:\n",
    "    # If genetic data is included, include gene4's AID values\n",
    "    aid_gene4 = set(imputed_gene4['AID'])\n",
    "else:\n",
    "    # If genetic data is not included, don't include gene4\n",
    "    aid_gene4 = set()\n",
    "\n",
    "# Debug: print the size of each AID set\n",
    "print(f\"AID values in df1: {len(aid_df1)}\")\n",
    "print(f\"AID values in df4: {len(aid_df4)}. Condition test_wave: {info_dict['test_wave']}\")\n",
    "print(f\"AID values in gene4: {len(aid_gene4)}. Condition include_genetic_data: {info_dict['include_genetic_data']}\")\n",
    "\n",
    "# Find the intersection of the relevant AID sets\n",
    "# Only calculate common_aids if we are using dataframes that are merged\n",
    "common_aids = aid_df1\n",
    "if aid_df4:  # Only consider df4 if it's being used\n",
    "    common_aids &= aid_df4\n",
    "if aid_gene4:  # Only consider gene4 if it's being used\n",
    "    common_aids &= aid_gene4\n",
    "\n",
    "# Debug: print the size of common_aids\n",
    "print(f\"Common AID values: {len(common_aids)}\")\n",
    "\n",
    "# Convert the result to a sorted list\n",
    "common_aids = sorted(common_aids)\n",
    "\n",
    "# Get column names for each dataframe\n",
    "columns_df1 = df1.columns.tolist()\n",
    "columns_df4 = df4.columns.tolist()\n",
    "columns_gene4 = imputed_gene4.columns.tolist()  # imputed\n",
    "\n",
    "# Merging DataFrames based on conditions\n",
    "if info_dict['test_wave'] == 'Wave I':\n",
    "    # Only merge df1 (no need to merge df4 or gene4)\n",
    "    if info_dict['include_genetic_data']:\n",
    "        # Merging df1 with imputed_gene4\n",
    "        df = pd.merge(df1, imputed_gene4, on='AID', how='outer')\n",
    "    else:\n",
    "        # Only include df1 (no genetic data)\n",
    "        df = df1.copy()\n",
    "elif info_dict['test_wave'] == 'Wave IV':\n",
    "    # Merge all three dataframes (df1, df4, and gene4)\n",
    "    if info_dict['include_genetic_data']:\n",
    "        # Merging df1, df4, and imputed_gene4\n",
    "        df = pd.merge(df1, df4, on='AID', how='outer')\n",
    "        df = pd.merge(df, imputed_gene4, on='AID', how='outer')\n",
    "\n",
    "    else:\n",
    "        # Merging df1 and df4 only (without genetic data)\n",
    "        df = pd.merge(df1, df4, on='AID', how='outer')\n",
    "\n",
    "\n",
    "# Debug: print the shape before filtering by common_aids\n",
    "print(f\"Shape before filtering: {df.shape}\")\n",
    "\n",
    "# Filter rows to keep only those with AID in common_aids\n",
    "df = df[df['AID'].isin(common_aids)]\n",
    "\n",
    "# Debug: print the shape after filtering by common_aids\n",
    "print(f\"Shape after filtering: {df.shape}\")\n",
    "\n",
    "# Reset the index for cleanliness\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Result\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# ONLY USE THIS IF ARE DOING THE SENSTIVITY ANALYSIS - PGS MDD\n",
    "## Extract the values, excluding 'FID' and 'AID' AND Major Depressive Disorder\n",
    "#columns_not_drop = [\"FID\", \"AID\",\"Major Depressive Disorder, 2019\"]\n",
    "#columns_to_drop_PGS = [value for value in dictw.values() if value not in columns_not_drop]\n",
    "#df = df.drop(columns=columns_to_drop_PGS)\n",
    "\n",
    "# ONLY USE THIS IF ARE DOING THE SENSTIVITY ANALYSIS - PHYSICAL HEALTH\n",
    "## Extract the values\n",
    "exclude_columns_physical = ['GENERAL HEALTH','DIFFICULTY USING HANDS/FT/LIMBS','FREQ-HEADACHES','FREQ-FEELING HOT',\n",
    "                           'FREQ-STOMACHACHE','FREQ-COLD SWEATS','FREQ-FEELING PHYSICALLY WEAK','FREQ-SORE THROAT/COUGH',\n",
    "                            'FREQ-PAINFUL/OFTEN URINATION','FREQ-FEELING VERY SICK',\n",
    "                           'FREQ-WAKE UP FEELING TIRED','FREQ-SKIN PROBLEMS','DIZZINESS','CHEST PAINS',\n",
    "                            'FREQ-MUSCLE/JOINT ACHES/PAINS']\n",
    "\n",
    "# Those were already removed: 'FREQ-VERY TIRED FOR NO REASON', 'FREQ-POOR APPETITE', 'FREQ-FEARFULNESS', ''REQ-TROUBLE RELAXING''\n",
    "#df = df.drop(columns=exclude_columns_physical)\n",
    "\n",
    "# ONLY USE THIS IF ARE DOING THE SENSTIVITY ANALYSIS - MENTAL HEALTH\n",
    "## Extract the values\n",
    "exclude_columns_mental = ['FREQ-INSOMNIA','NEEDED BUT NOT GET MEDICAL CARE',\n",
    "                         'WEIGHT IMAGE','TO GAIN/LOSE/MAINTAIN WEIGHT','PSYCHOLOGICAL COUNSELING','HEALTH CAUSE SCHOOL ABSENCE',\n",
    "                         'HEALTH CAUSE SOCIAL ABSENCE']\n",
    "#df = df.drop(columns=exclude_columns_mental)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e543bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecdbea6",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b72b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Spearman correlation matrix\n",
    "#corr_matrix = df.drop(columns=['AID', 'SEX', 'RACE', 'INCOME','category_W1A','category_W1B']).corr(method='spearman')\n",
    "\n",
    "\n",
    "# Mask the upper triangle of the correlation matrix\n",
    "#mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Visualize the correlation matrix using seaborn heatmap\n",
    "#plt.figure(figsize=(15, 10))\n",
    "#sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=1, mask=mask) \n",
    "   \n",
    "#plt.title('Spearman Correlation Matrix')\n",
    "#plt.show()\n",
    "\n",
    "# Create a set to store already printed pairs\n",
    "#printed_pairs = set()\n",
    "\n",
    "# Create a set to store columns to be dropped\n",
    "#columns_to_drop = set()\n",
    "\n",
    "# Find pairs with correlation above 0.8 and store the second variable of the pair for dropping\n",
    "#for col in corr_matrix.columns:\n",
    "#    for row in corr_matrix.index:\n",
    " #       # Ensure the correlation is above 0.8, not a self-correlation (i.e., row != col),\n",
    "  #      # and that the pair (row, col) hasn't been printed yet\n",
    "   #     if col != row and corr_matrix.loc[row, col] > 0.8:\n",
    "    #        # Create a tuple (min, max) to ensure the pair is processed only once\n",
    "     #       pair = tuple(sorted([row, col]))\n",
    "      #      if pair not in printed_pairs:\n",
    "       #         # Add the second variable in the pair (the one with the higher index) to the drop list\n",
    "        #        columns_to_drop.add(pair[1])\n",
    "         #       printed_pairs.add(pair)\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "#df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the columns that were dropped\n",
    "#print(f\"Columns dropped: {columns_to_drop}\")\n",
    "\n",
    "#print(corr_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f11370",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a17333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names\n",
    "print(\"Columns in df1:\", columns_df1)\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"Columns in df4:\", columns_df4)\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"Columns in gene4:\", columns_gene4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e572a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "\n",
    "# Convert any string representations of numbers to floats\n",
    "df_imputed = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Save the 'ID' column\n",
    "ID = df1['AID']\n",
    "\n",
    "\n",
    "# Features\n",
    "# List of columns to drop\n",
    "columns_to_drop = df4.columns.tolist() + ['AID', 'category_W1A', 'category_W1B', 'FID'] # Add more columns if needed\n",
    "\n",
    "# Drop the columns from df to create X\n",
    "X = df_imputed.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "y_A = df_imputed['category_W1A']  # Target variable using 19 predictors using Wave I\n",
    "y_B = df_imputed['category_W1B']  # Target variable using 10 predictors using Wave I\n",
    "if info_dict['test_wave'] == 'Wave IV':\n",
    "    y_4B = df_imputed['category_W4B'] # Target variable using 10 predictors using Wave IV\n",
    "\n",
    "if info_dict['clinically_depressed'] == 'adulthood':\n",
    "    y_4AH = df_imputed['adulthood_depression']\n",
    "\n",
    "if info_dict['clinically_depressed'] == 'adolescence':\n",
    "    y_4AC = df_imputed['adolescence_depression']\n",
    "\n",
    "print('features:',X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical\n",
    "# Start with all columns in df1 minus the specified columns\n",
    "categorical_cols = df1.columns.difference(['INCOME', 'category_W1B', 'category_W1A', 'AID']).tolist()\n",
    "\n",
    "     \n",
    "# Ensure there are no duplicates\n",
    "categorical_cols = list(set(categorical_cols))\n",
    "\n",
    "# Numeric\n",
    "# List of numerical columns (just 'income' initially)\n",
    "numerical_cols = ['INCOME']\n",
    "\n",
    "if info_dict['include_genetic_data']:\n",
    "    # Get the columns from gene4, excluding 'FID', 'AID', 'PSANCEST', 'PSRACE'\n",
    "    numerical_cols += imputed_gene4.columns.difference(['FID', 'AID', 'PSANCEST', 'PSRACE']).tolist() # imputed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceda37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate data by sex\n",
    "male_data = X[X['SEX'] == 1]\n",
    "female_data = X[X['SEX'] == 2]\n",
    "\n",
    "# Get columns in X that are also in gene4\n",
    "columns_in_gene4 = X.columns[X.columns.isin(imputed_gene4.columns)]\n",
    "\n",
    "# Get columns in X that are NOT in gene4\n",
    "columns_not_in_gene4 = X.columns[~X.columns.isin(imputed_gene4.columns)]\n",
    "\n",
    "# Calculate the percentage of NaN values for columns in gene4\n",
    "nan_percentage_gene4_male = male_data[columns_in_gene4].isna().mean() * 100\n",
    "nan_percentage_gene4_female = female_data[columns_in_gene4].isna().mean() * 100\n",
    "\n",
    "# Calculate the percentage of NaN values for columns NOT in gene4\n",
    "nan_percentage_not_gene4_male = male_data[columns_not_in_gene4].isna().mean() * 100\n",
    "nan_percentage_not_gene4_female = female_data[columns_not_in_gene4].isna().mean() * 100\n",
    "\n",
    "# Sort and select the top 10 columns with the highest percentage of missing values\n",
    "top_10_gene4_male = nan_percentage_gene4_male.sort_values(ascending=False).head(5)\n",
    "top_10_gene4_female = nan_percentage_gene4_female[top_10_gene4_male.index]\n",
    "\n",
    "top_10_not_gene4_male = nan_percentage_not_gene4_male.sort_values(ascending=False).head(5)\n",
    "top_10_not_gene4_female = nan_percentage_not_gene4_female[top_10_not_gene4_male.index]\n",
    "\n",
    "# Create a figure and set up two subplots (one on top of the other)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 14))\n",
    "\n",
    "# Plot for gene4 columns (Top 10) with male and female comparison\n",
    "width = 0.4  # Bar width\n",
    "x_gene4 = np.arange(len(top_10_gene4_male.index))  # X-axis positions\n",
    "\n",
    "axes[0].barh(x_gene4 - width / 2, top_10_gene4_male.values, height=width, label='Male', color='skyblue', edgecolor='black')\n",
    "axes[0].barh(x_gene4 + width / 2, top_10_gene4_female.values, height=width, label='Female', color='lightpink', edgecolor='black')\n",
    "axes[0].set_yticks(x_gene4)\n",
    "axes[0].set_yticklabels(top_10_gene4_male.index)\n",
    "axes[0].set_title('Top 5 Columns with Highest Percentage of Missing Values: Genetic')\n",
    "axes[0].set_xlabel('Percentage of Missing Values (%)')\n",
    "axes[0].set_ylabel('Columns')\n",
    "axes[0].grid(axis='x', linestyle='--')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot for non-gene4 columns (Top 10) with male and female comparison\n",
    "x_not_gene4 = np.arange(len(top_10_not_gene4_male.index))  # X-axis positions\n",
    "\n",
    "axes[1].barh(x_not_gene4 - width / 2, top_10_not_gene4_male.values, height=width, label='Male', color='skyblue', edgecolor='black')\n",
    "axes[1].barh(x_not_gene4 + width / 2, top_10_not_gene4_female.values, height=width, label='Female', color='lightpink', edgecolor='black')\n",
    "axes[1].set_yticks(x_not_gene4)\n",
    "axes[1].set_yticklabels(top_10_not_gene4_male.index)\n",
    "axes[1].set_title('Top 5 Columns with Highest Percentage of Missing Values: Environmental')\n",
    "axes[1].set_xlabel('Percentage of Missing Values (%)')\n",
    "axes[1].set_ylabel('Columns')\n",
    "axes[1].grid(axis='x', linestyle='--')\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust the layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in columns specified in categorical_cols\n",
    "for col in categorical_cols:\n",
    "    if col in X.columns:  # Ensure the column exists in X\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])  # Fill NaN with the most frequent value (mode)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Initialize KNNImputer with the desired number of neighbors (e.g., 5)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Only use this if you are conductiing the senstitivy analysis\n",
    "# Filter numerical_cols to include only those that exist in X\n",
    "numerical_cols_existing = [col for col in numerical_cols if col in X.columns]\n",
    "\n",
    "# Apply the KNN imputer on the numerical columns\n",
    "#X[numerical_cols] = knn_imputer.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Only use this if you are conductiing the senstitivy analysis\n",
    "# Apply the KNN imputer on the numerical columns that are present in X\n",
    "X[numerical_cols_existing] = knn_imputer.fit_transform(X[numerical_cols_existing])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e28846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize numeric data (PCAs are normalized already)\n",
    "scaler = MinMaxScaler()\n",
    "X['INCOME'] = scaler.fit_transform(X[['INCOME']])\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If comparing with clinical depression\n",
    "if info_dict['clinically_depressed'] == 'adulthood' or info_dict['clinically_depressed'] == 'adolescence':\n",
    "    if info_dict['clinically_depressed'] == 'adulthood':\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_4AH, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_4AC, test_size=0.2, random_state=42)\n",
    "    # Cross-validation setup using the combined stratification column\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # XGBoost hyperparameter tuning\n",
    "    xgb_param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "        'subsample': [0.5, 0.8, 1.0],\n",
    "        'max_features': [5, 10, 15, 20, 25]\n",
    "    }\n",
    "    xgb = XGBClassifier(eval_metric='logloss', random_state=42, verbosity=0)\n",
    "    xgb_grid = GridSearchCV(xgb, xgb_param_grid, cv=kfold, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "    xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model from the grid search\n",
    "    best_xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "    # Function to calculate Confidence Interval\n",
    "    def calculate_confidence_interval(scores, confidence=0.95):\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        n = len(scores)\n",
    "        t_critical = stats.t.ppf((1 + confidence) / 2., n - 1)\n",
    "        margin_of_error = t_critical * (std_score / np.sqrt(n))\n",
    "        return mean_score - margin_of_error, mean_score + margin_of_error\n",
    "\n",
    "    # Store results for XGBoost\n",
    "    names, auc_means, auc_ci_lowers, auc_ci_uppers, accuracy_means = [], [], [], [], []\n",
    "    accuracy_ci_lowers, accuracy_ci_uppers, aucs = [], [], []\n",
    "\n",
    "    # Train and evaluate the best XGBoost model\n",
    "    model_name = 'XGBoost'\n",
    "    model = best_xgb_model\n",
    "    \n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on test set\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Perform bootstrapping for confidence intervals\n",
    "    auc, auc_mean, auc_ci_lower, auc_ci_upper, accuracy_mean, accuracy_ci_lower, accuracy_ci_upper = bootstrap_metrics(y_test, y_prob)\n",
    "\n",
    "    # Store results\n",
    "    aucs.append(auc)\n",
    "    names.append(model_name)\n",
    "    auc_means.append(auc_mean)\n",
    "    auc_ci_lowers.append(auc_ci_lower)\n",
    "    auc_ci_uppers.append(auc_ci_upper)\n",
    "    accuracy_means.append(accuracy_mean)\n",
    "    accuracy_ci_lowers.append(accuracy_ci_lower)\n",
    "    accuracy_ci_uppers.append(accuracy_ci_upper)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{model_name} Test AUC: {auc_mean:.3f} (CI: {auc_ci_lower:.3f} - {auc_ci_upper:.3f})\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    importances = model.get_booster().get_score(importance_type='gain')\n",
    "    \n",
    "    # Convert to sorted list of (feature, importance)\n",
    "    sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    features = [item[0] for item in sorted_importances]\n",
    "    gains = [item[1] for item in sorted_importances]\n",
    "\n",
    "    # Format bars\n",
    "    plt.figure(figsize=(12, 8), dpi=800)\n",
    "    bars = plt.barh(range(len(features)), gains, color='#1f77b4')\n",
    "    plt.yticks(range(len(features)), features)\n",
    "    plt.gca().invert_yaxis()  # Highest at the top\n",
    "    plt.xlabel('Gain')\n",
    "    plt.title('Top 10 Feature Importances (Gain) - XGBoost')\n",
    "\n",
    "    # Add formatted values to bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_width() + max(gains)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{gains[i]:.0f}', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    sys.exit()  # This will stop the program here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84ee80",
   "metadata": {},
   "source": [
    "# Simple Neural Network (classification) W1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the 'single' model (fast model)\n",
    "def run_single_model(X_train, y_train):\n",
    "    # Build the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Define the input shape\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "                  optimizer=Adam(),            # Adam optimizer\n",
    "                  metrics=['accuracy'])        # Track accuracy during training\n",
    "\n",
    "    # Train the model with validation\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=32, \n",
    "        validation_split=0.2,   # Use 20% of the training data as validation data\n",
    "        verbose=1               # Print progress during training\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dd74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the 'multiple' model with hyperparameter tuning\n",
    "def run_multiple_models(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    class MyBinaryClassifierHyperModel(HyperModel):\n",
    "        def build(self, hp):\n",
    "            model = tf.keras.Sequential()\n",
    "\n",
    "            # Tuning the number of hidden layers\n",
    "            for i in range(hp.Int('num_layers', 2, 6)):  # Number of hidden layers between 2 and 6\n",
    "                model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32),\n",
    "                                                activation='relu'))\n",
    "\n",
    "            # Add Dropout layer after each Dense layer\n",
    "            model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.2, max_value=0.5, step=0.05)))\n",
    "\n",
    "            # Output layer for binary classification\n",
    "            model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Single output with sigmoid activation\n",
    "\n",
    "            # Compile the model with tunable learning rate\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "                          loss='binary_crossentropy',  # Binary classification loss\n",
    "                          metrics=['accuracy'])#, 'AUC', 'Precision', 'Recall'])\n",
    "\n",
    "            return model\n",
    "\n",
    "    # Instantiate the hypermodel for binary classification\n",
    "    hypermodel = MyBinaryClassifierHyperModel()\n",
    "\n",
    "    # Set up the tuner with RandomSearch\n",
    "    tuner = RandomSearch(\n",
    "        hypermodel,\n",
    "        objective='val_accuracy',  # We use validation accuracy to find the best model\n",
    "        max_trials=100,  # The number of different hyperparameter combinations to try\n",
    "        executions_per_trial=3,  # Number of models trained per trial (for stability)\n",
    "        directory='hyperparameters-log',  # Directory where tuning results will be saved\n",
    "        project_name='binary_classifier_tuning'  # Name of the tuning project\n",
    "    )\n",
    "\n",
    "       # Check shapes of the splits\n",
    "    print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "    print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "    # Start the hyperparameter search on your dataset\n",
    "    tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Get the best model after tuning\n",
    "    model = tuner.get_best_models(1)[0]\n",
    "\n",
    "    # Optionally, you can evaluate the best model\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y, test_size=0.2, val_size=0.1, random_state=42, include_genetic_data=True, gene4=None,df1=None, sex_gender=None,include_environmental_data=True):\n",
    "    \"\"\"\n",
    "    Updates X and y, handles missing values, and splits the data into train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): The feature dataset.\n",
    "    - y (pd.DataFrame): The target.\n",
    "    - test_size (float): Proportion of data to allocate to test and validation sets.\n",
    "    - val_size (float): Proportion of total data to allocate to the validation set.\n",
    "    - random_state (int): Random state for reproducibility.\n",
    "    - include_genetic_data (bool): Whether to include genetic data in X.\n",
    "    - gene4 (pd.DataFrame): The dataframe containing genetic information columns to exclude.\n",
    "    - sex_gender (str): Filter by gender (\"male\" or \"female\").\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_val, X_test: Feature splits for train, validation, and test sets.\n",
    "    - y_train, y_val, y_test: Target splits for train, validation, and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter data by sex_gender if specified\n",
    "    if sex_gender == \"Male\":\n",
    "        X = X[X['SEX'] == 1]\n",
    "        # Drop female-specific columns\n",
    "        X = X.drop(columns=[\n",
    "        \"FREQ-MENSTRUAL CRAMPS\",\n",
    "        \"FORCED SEX\",\n",
    "        \"PREGNANCY OUTCOME\",\n",
    "        \"Menarche, 2014\",\n",
    "        \"Menopause, 2015\",\n",
    "        \"SEX\"], errors=\"ignore\")  # errors=\"ignore\" prevents errors if a column is missing\n",
    "\n",
    "    elif sex_gender == \"Female\":\n",
    "        X = X[X['SEX'] == 2]\n",
    "        X = X.drop(columns=[\"SEX\"], errors=\"ignore\")  # errors=\"ignore\" prevents errors if a column is missing\n",
    "\n",
    "    # Ensure y aligns with X after filtering\n",
    "    y = y.loc[X.index]\n",
    "    \n",
    "    \n",
    "    # Conditionally include or remove genetic data\n",
    "    if not include_genetic_data and gene4 is not None:\n",
    "        # Remove all columns in gene4 except 'AID'\n",
    "        genetic_columns = [col for col in gene4.columns if col != 'AID']\n",
    "        X = X.drop(columns=genetic_columns, errors='ignore')\n",
    "\n",
    "    # Conditionally include or remove enviromental data\n",
    "    if not include_environmental_data and df1 is not None:\n",
    "        # Remove all columns in df1 except 'AID'\n",
    "        environmnetal_columns = [col for col in df1.columns if col != 'AID']\n",
    "        X = X.drop(columns=environmnetal_columns, errors='ignore')\n",
    "\n",
    "   \n",
    "    # Split into training and temp (test + validation)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Split the temp set into validation and test sets\n",
    "    val_test_ratio = val_size / (test_size)  # Proportion of temp data for validation\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1 - val_test_ratio, random_state=random_state)\n",
    "\n",
    "    # Check shapes of the splits\n",
    "    print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "    print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebef890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to choose between 'single' and 'multiple' models\n",
    "def choose_model(X_train, y_train, X_val=None, y_val=None, X_test=None, y_test=None, model_type='single'):\n",
    "    \"\"\"\n",
    "    Choose and run either a single model or multiple models based on the input parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train: Training data.\n",
    "    - X_val, y_val: Validation data (optional for single model).\n",
    "    - X_test, y_test: Test data (optional for single model).\n",
    "    - model_type (str): Type of model to run ('single' or 'multiple').\n",
    "\n",
    "    Returns:\n",
    "    - model: The trained model(s).\n",
    "    - history: Training history (for single model).\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_type == 'single':\n",
    "        model, history = run_single_model(X_train,y_train)\n",
    "        return model, history\n",
    "    elif model_type == 'multiple':\n",
    "        model = run_multiple_models(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose either 'single' or 'multiple'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ac44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the entire DataFrame contains any NaN values\n",
    "X.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe6939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if info_dict['CESD_type'] == 'A':\n",
    "    y = y_A\n",
    "elif info_dict['CESD_type'] == 'B':\n",
    "    y = y_B\n",
    "else:\n",
    "    ValueError(\"Invalid CESD_type. Choose either 'A' or 'B'.\")\n",
    "\n",
    "\n",
    "# Drop RACE\n",
    "#X = X.drop(columns=['RACE'],errors='ignore')\n",
    "\n",
    "X4 = X\n",
    "# Filter X4 data by sex_gender if specified (if info_dict['test_wave'] == 'Wave IV')\n",
    "if info_dict['test_wave'] == 'Wave IV':\n",
    "    if info_dict['sex_gender'] == 'Male':\n",
    "        y_4B = y_4B[X4['SEX'] == 1]\n",
    "        \n",
    "        X4 = X4[X4['SEX'] == 1]\n",
    "        # Drop female-specific columns\n",
    "        X4 = X4.drop(columns=[\n",
    "            \"FREQ-MENSTRUAL CRAMPS\",\n",
    "            \"FORCED SEX\",\n",
    "            \"PREGNANCY OUTCOME\",\n",
    "            \"Menarche, 2014\",\n",
    "            \"Menopause, 2015\",\n",
    "            \"SEX\"], errors=\"ignore\")  # errors=\"ignore\" prevents errors if a column is missing\n",
    "        \n",
    "    elif info_dict['sex_gender'] == 'Female':\n",
    "        y_4B = y_4B[X4['SEX'] == 2]\n",
    "        X4 = X4[X4['SEX'] == 2]\n",
    "        X4 = X4.drop(columns=[\"SEX\"], errors=\"ignore\")  # errors=\"ignore\" prevents errors if a column is missing\n",
    "    \n",
    "print('Target choosen:',info_dict['CESD_type'])\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(X,y,test_size=0.2, val_size=0.1, random_state=42,\n",
    "                                                              include_genetic_data=info_dict['include_genetic_data'],\n",
    "                                                              gene4=gene4,\n",
    "                                                              df1=df1,\n",
    "                                                             sex_gender = info_dict['sex_gender'],\n",
    "                                                             include_environmental_data=info_dict['include_environmental_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6370e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Choose 'single' (just one model training) or 'multiple' model (turner search)\n",
    "model,history = choose_model(X_train, y_train, X_val, y_val, X_test, y_test, model_type=info_dict['model_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model architecture to confirm it's correct for binary classification\n",
    "model_nn = model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e052be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots: 1 row, 2 columns\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot training & validation accuracy values on the first subplot\n",
    "ax[0].plot(history.history['accuracy'], label='Train')\n",
    "ax[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[0].legend(loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values on the second subplot\n",
    "ax[1].plot(history.history['loss'], label='Train')\n",
    "ax[1].plot(history.history['val_loss'], label='Validation')\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[1].legend(loc='upper left')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a30e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the model's output with a small batch of test data\n",
    "output = model.predict(X_train[:5])\n",
    "print(\"Model Output (Predicted Probabilities):\")\n",
    "for sublist in output:\n",
    "    print([f\"{sublist[0]:.6f}\"])\n",
    "\n",
    "y_pred_nn = [sublist[0] for sublist in model.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e91dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"Symptoms of Depression Classification (E1{info_dict['CESD_type']}  -> D1{info_dict['CESD_type']})\\\n",
    "          \\nTarget Train and Validation: {info_dict['test_wave']} \\\n",
    "          \\nModel Type: {info_dict['model_type']} \\\n",
    "          \\nInclude environmental files: {info_dict['include_environmental_data']} \\\n",
    "          \\nInclude genetic files: {info_dict['include_genetic_data']} \\\n",
    "          \\nGender: {info_dict['sex_gender']} \\\n",
    "          \\nTesting Size: {len(y_test)}\"\n",
    "if info_dict[\"test_wave\"] == 'Wave I':\n",
    "    aucs_nn ,auc_mean_nn, auc_lower_nn, auc_upper_nn, acc_mean_nn, acc_lower_nn, acc_upper_nn = plot_confusion_matrix_and_roc(y_test,\n",
    "                                model,\n",
    "                                X_test,\n",
    "                                title,\n",
    "                                info_dict=info_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d1b9e",
   "metadata": {},
   "source": [
    "## Cross generation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using the full Wave IV to predict even the ones that I use to train (I think it is fine because is another wave)\n",
    "if info_dict['test_wave'] == 'Wave IV':\n",
    "  title = f\"Symptoms of Depression Classification (Cross Waves E1{info_dict['CESD_type']} -> D4B) \\\n",
    "          \\nTarget Train and Validation: {info_dict['test_wave']} \\\n",
    "        \\nModel Type: {info_dict['model_type']}\\nInclude genetic files: {info_dict['include_genetic_data']} \\\n",
    "        \\n Gender: {info_dict['sex_gender']}\\\n",
    "        \\nTesting Size: {len(y_4B)}\" \n",
    "\n",
    "if info_dict[\"test_wave\"] == 'Wave IV':\n",
    "    aucs_nn ,auc_mean_nn, auc_lower_nn, auc_upper_nn, acc_mean_nn, acc_lower_nn, acc_upper_nn = plot_confusion_matrix_and_roc(y_4B,\n",
    "                                model,\n",
    "                                X4,\n",
    "                                title,\n",
    "                                info_dict=info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578d80b",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fea2c5",
   "metadata": {},
   "source": [
    "## Bewwsarm/sumary plot\n",
    "- Features on the Y-axis:\n",
    "    - Each dot represents a feature in your dataset. \n",
    "    - The Y-axis will list your input features.\n",
    "- SHAP values on the X-axis:\n",
    "    - SHAP values represent the contribution of each feature to the model’s prediction for a particular instance. \n",
    "        - Positive SHAP values push the prediction higher (towards 1, since it's a binary classification with sigmoid activation) \n",
    "        - Negative SHAP values push it lower (towards 0).\n",
    "    - A feature that has a large positive SHAP value means it contributed significantly to the model predicting a positive class (1) high depression symptoms. \n",
    "    - A feature with a large negative SHAP value means it pushed the model's prediction towards the negative class (0) lower depression symptoms.\n",
    "- Dot Density and Spread:\n",
    "    - The distribution of dots on the plot will show the variation of the feature's SHAP values across all instances in your dataset.\n",
    "    - Features with a wider spread and more varied SHAP values are more influential across different instances, while features with a narrow spread might be less impactful overall.\n",
    "    - If you see a lot of dots concentrated far to the right (positive SHAP values), it means that feature usually contributes positively towards predicting the positive class.\n",
    "\n",
    "- Coloring:\n",
    "\n",
    "    - Typically, the color of each dot corresponds to the feature's value (or another related aspect). For example, red might indicate higher values of a feature, while blue might represent lower values. This color gradient helps you see how the value of the feature correlates with the direction of the SHAP value (whether it’s pushing the model prediction higher or lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use a small subset of data to speed up SHAP computation\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test[:500])  # Compute SHAP values for 100 samples\n",
    "\n",
    "# Plot feature importance\n",
    "shap.summary_plot(shap_values, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[101], max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4df9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronfenbrenner_dict = {\n",
    "    # Individual (Personal Attributes & Health)\n",
    "    'SEX': 'Individual',\n",
    "    'RACE': 'Individual',\n",
    "    'GENERAL HEALTH': 'Individual',\n",
    "    'DIFFICULTY USING HANDS/FT/LIMBS': 'Individual',\n",
    "    'FREQ-HEADACHES': 'Individual',\n",
    "    'FREQ-FEELING HOT': 'Individual',\n",
    "    'FREQ-STOMACHACHE': 'Individual',\n",
    "    'FREQ-COLD SWEATS': 'Individual',\n",
    "    'FREQ-FEELING PHYSICALLY WEAK': 'Individual',\n",
    "    'FREQ-SORE THROAT/COUGH': 'Individual',\n",
    "    'FREQ-VERY TIRED FOR NO REASON': 'Individual',\n",
    "    'painful or very frequent urination (or peeing)': 'Individual',\n",
    "    'feeling really sick': 'Individual',\n",
    "    'FREQ-WAKE UP FEELING TIRED': 'Individual',\n",
    "    'skin problems, such as itching or pimples': 'Individual',\n",
    "    'dizziness': 'Individual',\n",
    "    'chest pains': 'Individual',\n",
    "    'aches, pains, or soreness in your muscles or joints': 'Individual',\n",
    "    'FREQ-MENSTRUAL CRAMPS': 'Individual',\n",
    "    'FREQ-POOR APPETITE': 'Individual',\n",
    "    'FREQ-INSOMNIA': 'Individual',\n",
    "    'FREQ-TROUBLE RELAXING': 'Individual',\n",
    "    'FREQ-FEARFULNESS': 'Individual',\n",
    "    'WEIGHT IMAGE': 'Individual',\n",
    "    'TO GAIN/LOSE/MAINTAIN WEIGHT': 'Individual',\n",
    "    'LIKE SELF AS ARE': 'Individual',\n",
    "    'DO EVERYTHING JUST RIGHT': 'Individual',\n",
    "    'FEEL SOCIALLY ACCEPTED': 'Individual',\n",
    "    'FEEL LOVED AND WANTED': 'Individual',\n",
    "\n",
    "    # Microsystem (Immediate Relationships & Environments)\n",
    "    'CLOSE TO MOM': 'Microsystem',\n",
    "    'MOM-HOW MUCH DOES SHE CARE': 'Microsystem',\n",
    "    'CLOSE TO DAD': 'Microsystem',\n",
    "    'DAD-HOW MUCH DOES HE CARE': 'Microsystem',\n",
    "    'MOM-WARM AND LOVING': 'Microsystem',\n",
    "    'MOM-GOOD COMMUNICATION': 'Microsystem',\n",
    "    'MOM-GOOD RELATIONSHIP': 'Microsystem',\n",
    "    'DAD-GOOD COMMUNICATION': 'Microsystem',\n",
    "    'DAD-GOOD RELATIONSHIP': 'Microsystem',\n",
    "    'FAMILY UNDERSTAND YOU': 'Microsystem',\n",
    "    'FAMILY PAYS ATTENTION TO YOU': 'Microsystem',\n",
    "    'FAMILY HAS FUN TOGETHER': 'Microsystem',\n",
    "    'PARENTS CARE ABOUT YOU': 'Microsystem',\n",
    "    'WANT TO LEAVE HOME': 'Microsystem',\n",
    "    'HANG OUT WITH FRIENDS': 'Microsystem',\n",
    "    'FRIENDS CARE ABOUT YOU': 'Microsystem',\n",
    "    'PAST YR-FRIENDS ATTEMPT SUICIDE': 'Microsystem',\n",
    "    'PAST YR-FRIENDS SUCCEED': 'Microsystem',\n",
    "    'You feel close to people at your school.': 'Microsystem',\n",
    "    'FEEL PART OF YOUR SCHOOL': 'Microsystem',\n",
    "    'STUDENTS AT SCHOOL PREJUDICED': 'Microsystem',\n",
    "    'HAPPY AT YOUR SCHOOL': 'Microsystem',\n",
    "    'The teachers at your school treat students fairly.': 'Microsystem',\n",
    "    'FEEL SAFE IN YOUR SCHOOL': 'Microsystem',\n",
    "    'TEACHERS CARE ABOUT YOU': 'Microsystem',\n",
    "    'TROUBLE-GETTING ALONG TEACHERS': 'Microsystem',\n",
    "    'paying attention in school?': 'Microsystem',\n",
    "    'TROUBLE-GETTING HOMEWORK DONE': 'Microsystem',\n",
    "    'TROUBLE-WITH OTHER STUDENTS': 'Microsystem',\n",
    "\n",
    "    # Mesosystem (Interactions Between Microsystems)\n",
    "    'FAMILY UNDERSTAND YOU': 'Mesosystem',\n",
    "    'FAMILY PAYS ATTENTION TO YOU': 'Mesosystem',\n",
    "    'TEACHERS CARE ABOUT YOU': 'Mesosystem',\n",
    "    'TROUBLE-WITH OTHER STUDENTS': 'Mesosystem',\n",
    "    'PAST YR-FRIENDS ATTEMPT SUICIDE': 'Mesosystem',\n",
    "\n",
    "    # Exosystem (Indirect Environmental Influences)\n",
    "    'INCOME': 'Exosystem',\n",
    "    'ENOUGH MONEY FOR BILLS': 'Exosystem',\n",
    "    'MEDICAL CARE FOR FAMILY': 'Exosystem',\n",
    "    'NEEDED BUT NOT GET MEDICAL CARE': 'Exosystem',\n",
    "    'WHY-COULD NOT PAY': 'Exosystem',\n",
    "    'FEEL SAFE IN NBORHOOD?': 'Exosystem',\n",
    "    'HOW HAPPY LIVING IN NBORHOOD': 'Exosystem',\n",
    "    'SAW SHOOTING/STABBING OF PERSON': 'Exosystem',\n",
    "    'HAD KNIFE/GUN PULLED ON YOU': 'Exosystem',\n",
    "    'SOMEONE SHOT YOU': 'Exosystem',\n",
    "    'SOMEONE STABBED YOU': 'Exosystem',\n",
    "    'GOT INTO A PHYSICAL FIGHT': 'Exosystem',\n",
    "    'WERE JUMPED': 'Exosystem',\n",
    "    'PAST YR-FAMILY ATTEMPT SUICIDE': 'Exosystem',\n",
    "    'PAST YR-FAMILY SUCCEED': 'Exosystem',\n",
    "\n",
    "    # Macrosystem (Cultural & Societal Influences)\n",
    "    'FORCED SEX': 'Macrosystem',\n",
    "    'SEX FOR DRUGS OR MONEY': 'Macrosystem',\n",
    "\n",
    "    # Chronosystem (Life Events & Changes Over Time)\n",
    "    'PREG1 PREGNANCY OUTCOME': 'Chronosystem',\n",
    "    'HEALTH CAUSE SCHOOL ABSENCE': 'Chronosystem',\n",
    "    'HEALTH CAUSE SOCIAL ABSENCE': 'Chronosystem',\n",
    "    'PAST YR-FRIENDS ATTEMPT SUICIDE': 'Chronosystem',\n",
    "    'PAST YR-FRIENDS SUCCEED': 'Chronosystem',\n",
    "    'PAST YR-FAMILY ATTEMPT SUICIDE': 'Chronosystem',\n",
    "    'PAST YR-FAMILY SUCCEED': 'Chronosystem'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming shap_values is your SHAP Explanation object\n",
    "shap_values_array = shap_values.values  # Extract SHAP values\n",
    "\n",
    "# Compute the mean absolute SHAP values for each feature\n",
    "mean_abs_shap_values = np.abs(shap_values_array).mean(axis=0)\n",
    "\n",
    "# Get the feature names (columns of X_train)\n",
    "features = X_train.columns  # Assuming X_train is your DataFrame\n",
    "\n",
    "# Sort the mean absolute SHAP values in descending order\n",
    "sorted_idx = np.argsort(mean_abs_shap_values)[::-1]  # Indices that would sort the values in descending order\n",
    "sorted_values = mean_abs_shap_values[sorted_idx]\n",
    "sorted_features = features[sorted_idx]\n",
    "\n",
    "# Select only the top 10 features\n",
    "top_10_values = sorted_values[:10]\n",
    "top_10_features = sorted_features[:10]\n",
    "\n",
    "# Define color mapping for Bronfenbrenner categories\n",
    "category_colors = {\n",
    "    'Individual': 'lightblue',\n",
    "    'Microsystem': 'lightgreen',\n",
    "    'Mesosystem': 'mediumseagreen',\n",
    "    'Exosystem': 'purple',\n",
    "    'Macrosystem': 'orange',\n",
    "    'Chronosystem': 'red'\n",
    "}\n",
    "\n",
    "# Assuming 'bronfenbrenner_dict' is available and maps feature names to categories\n",
    "# Determine colors based on df1 membership and Bronfenbrenner category\n",
    "colors = []\n",
    "labels = []\n",
    "\n",
    "for feature in top_10_features:\n",
    "    if feature in bronfenbrenner_dict:\n",
    "        # If the feature is in bronfenbrenner_dict, use the corresponding color\n",
    "        category = bronfenbrenner_dict.get(feature, 'gold')\n",
    "        colors.append(category_colors.get(category, 'gold'))  # Default to 'gold' if category not found\n",
    "        labels.append(category)  # Use the category as label\n",
    "    else:\n",
    "        # If the feature is not in the bronfenbrenner_dict, color it gold and label it \"Polygenic Scores\"\n",
    "        colors.append('gold')\n",
    "        labels.append('Polygenic Scores')\n",
    "\n",
    "# Create the horizontal bar plot for top 10 with color mapping\n",
    "plt.figure(figsize=(10, 8))\n",
    "bars = plt.barh(range(len(top_10_values)), top_10_values, color=colors, edgecolor='black')\n",
    "\n",
    "# Label the axes and set the title\n",
    "plt.ylabel('Feature')\n",
    "plt.xlabel('Mean Absolute SHAP Value')\n",
    "plt.title('Top 10 Features by Mean Absolute SHAP Value')\n",
    "\n",
    "# Set the feature names as y-tick labels (larger values are at the top)\n",
    "plt.yticks(range(len(top_10_values)), top_10_features)  # Use top_10_features as y-tick labels\n",
    "\n",
    "# Optionally, add legend to indicate categories (based on the colors)\n",
    "handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in category_colors.values()] + [plt.Rectangle((0, 0), 1, 1, color='gold')]\n",
    "labels = list(category_colors.keys()) + ['Polygenic Scores']\n",
    "\n",
    "# Move the legend to the top-right corner inside the plot\n",
    "plt.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.95, 0.95))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c37574",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ce978",
   "metadata": {},
   "source": [
    "# Decision Tree / XGBoost / SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress XGBoost warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "os.environ['XGBOOST_ENABLE_WARNINGS'] = '0'\n",
    "\n",
    "\n",
    "# Cross-validation setup using the combined stratification column\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# XGBoost hyperparameter tuning\n",
    "xgb_param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'max_features': [5, 10, 15, 20, 25]\n",
    "}\n",
    "xgb = XGBClassifier(eval_metric='logloss', random_state=42, verbosity=0)\n",
    "xgb_grid = GridSearchCV(xgb, xgb_param_grid, cv=kfold, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Initialize models with tuned XGBoost\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(min_samples_leaf=150, min_samples_split=75, random_state=42),\n",
    "    'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'XGBoost': xgb_grid.best_estimator_\n",
    "}\n",
    "\n",
    "# Function to calculate Confidence Interval\n",
    "def calculate_confidence_interval(scores, confidence=0.95):\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    n = len(scores)\n",
    "    t_critical = stats.t.ppf((1 + confidence) / 2., n - 1)\n",
    "    margin_of_error = t_critical * (std_score / np.sqrt(n))\n",
    "    return mean_score - margin_of_error, mean_score + margin_of_error\n",
    "\n",
    "# Store results\n",
    "names, auc_means, auc_ci_lowers, auc_ci_uppers, accuracy_means = [], [], [], [], []\n",
    "accuracy_ci_lowers, accuracy_ci_uppers, aucs, y_pred_list = [], [], [], []\n",
    "\n",
    "# If we are analysing Wave IV, the test set is all of the values in Wave IV, because we trained in Wave I.\n",
    "if info_dict[\"test_wave\"] == 'Wave IV':\n",
    "    X_test = X4\n",
    "    y_test = y_4B\n",
    " \n",
    "for name, model in tqdm(models.items(), desc=\"Training models\"):\n",
    "    model.fit(X_train, y_train)  # Train model\n",
    "\n",
    "    \n",
    "    # Get predictions on test set\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Perform bootstrapping for confidence intervals\n",
    "    auc, auc_mean, auc_ci_lower, auc_ci_upper, accuracy_mean, accuracy_ci_lower, accuracy_ci_upper = bootstrap_metrics(y_test, y_prob)\n",
    "    \n",
    "    # Store results\n",
    "    aucs.append(auc)\n",
    "    names.append(name)\n",
    "    auc_means.append(auc_mean)\n",
    "    auc_ci_lowers.append(auc_ci_lower)\n",
    "    auc_ci_uppers.append(auc_ci_upper)\n",
    "    accuracy_means.append(accuracy_mean)\n",
    "    accuracy_ci_lowers.append(accuracy_ci_lower)\n",
    "    accuracy_ci_uppers.append(accuracy_ci_upper)\n",
    "    y_pred_list.append(y_prob.tolist())\n",
    "    print(y_prob)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name} Test AUC: {auc_mean:.3f} (CI: {auc_ci_lower:.3f} - {auc_ci_upper:.3f})\")\n",
    "    print(f\"{name} Test Accuracy: {accuracy_mean:.3f} (CI: {accuracy_ci_lower:.3f} - {accuracy_ci_upper:.3f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plot the decision tree with larger figure size\n",
    "plt.figure(figsize=(120, 50), dpi=200)  # Increase figure size\n",
    "plot_tree(models['Decision Tree'], \n",
    "          filled=True, \n",
    "          feature_names=X_train.columns, \n",
    "          class_names=['0', '1'], \n",
    "          fontsize=10,   # Font size\n",
    "          rounded=True,  # Rounded corners\n",
    "          proportion=True)  # Proportional sizes of the tree nodes\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fb993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model_logistic_regression = LogisticRegression()\n",
    "model_logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and get predicted probabilities\n",
    "y_test_pred_lr = model_logistic_regression.predict(X_test)\n",
    "y_test_prob_lr = model_logistic_regression.predict_proba(X_test)[:, 1]  # Probabilities for AUC calculation\n",
    "\n",
    "auc_lr, auc_mean_lr, auc_ci_lower_lr, auc_ci_upper_lr, accuracy_mean_lr, accuracy_ci_lower_lr, accuracy_ci_upper_lr = bootstrap_metrics(y_test, y_test_prob_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d3e97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if \"Logistic Regression\" is not already in the names list\n",
    "if \"Logistic Regression\" not in names:\n",
    "    models = {\"Logistic Regression\": model_logistic_regression, **models}\n",
    "    names.insert(0, \"Logistic Regression\")  \n",
    "    aucs.insert(0, auc_lr) \n",
    "    auc_means.insert(0, auc_mean_lr)  \n",
    "    auc_ci_lowers.insert(0, auc_ci_lower_lr)  \n",
    "    auc_ci_uppers.insert(0, auc_ci_upper_lr)  \n",
    "    accuracy_means.insert(0, accuracy_mean_lr)  \n",
    "    accuracy_ci_lowers.insert(0, accuracy_ci_lower_lr)  \n",
    "    accuracy_ci_uppers.insert(0, accuracy_ci_upper_lr)\n",
    "    y_pred_list.insert(0, y_test_prob_lr)\n",
    "\n",
    "\n",
    "# Check if \"Neural Network\" is not already in the names list\n",
    "if \"Neural Network\" not in names:\n",
    "    models[\"Neural Network\"] = model_nn\n",
    "    names.append(\"Neural Network\")\n",
    "    aucs.append(aucs_nn)\n",
    "    auc_means.append(auc_mean_nn)\n",
    "    auc_ci_lowers.append(auc_lower_nn)\n",
    "    auc_ci_uppers.append(auc_upper_nn)\n",
    "    accuracy_means.append(acc_mean_nn)\n",
    "    accuracy_ci_lowers.append(acc_lower_nn)\n",
    "    accuracy_ci_uppers.append(acc_upper_nn)\n",
    "    y_pred_list.append(y_pred_nn)\n",
    "\n",
    "# Compute error bars\n",
    "auc_errors = [(auc_means[i] - auc_ci_lowers[i], auc_ci_uppers[i] - auc_means[i]) for i in range(len(names))]\n",
    "accuracy_errors = [(accuracy_means[i] - accuracy_ci_lowers[i], accuracy_ci_uppers[i] - accuracy_means[i]) for i in range(len(names))]\n",
    "\n",
    "auc_errors = np.array(auc_errors).T\n",
    "accuracy_errors = np.array(accuracy_errors).T\n",
    "\n",
    "# Plot side-by-side figures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "x_pos = np.arange(len(names))  # Position of bars\n",
    "\n",
    "# Accuracy plot (left)\n",
    "bars_acc = axes[0].bar(x_pos, accuracy_means, yerr=accuracy_errors, capsize=5, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(\"Model Accuracy\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_xticks(x_pos)  # Set tick positions\n",
    "axes[0].set_xticklabels(names, rotation=45, ha='right')  # Set tick labels\n",
    "axes[0].set_ylim(0.725, .875)\n",
    "\n",
    "# Add the values on top of each bar in the accuracy plot\n",
    "for i, bar in enumerate(bars_acc):\n",
    "    yval = bar.get_height()\n",
    "    error_top = accuracy_errors[1, i]  # Get the upper error bar for this bar\n",
    "    axes[0].text(bar.get_x() + bar.get_width() / 2, yval + error_top  - 0.05,  # Slight offset above error bar\n",
    "                 f'{yval:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# AUC plot (right)\n",
    "bars_auc = axes[1].bar(x_pos, auc_means, yerr=auc_errors, capsize=5, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(\"Model AUC\")\n",
    "axes[1].set_ylabel(\"AUC\")\n",
    "axes[1].set_xticks(x_pos)  # Set tick positions\n",
    "axes[1].set_xticklabels(names, rotation=45, ha='right')  # Set tick labels\n",
    "axes[1].set_ylim(0.7, .9)\n",
    "\n",
    "# Add the values on top of each bar in the plot\n",
    "for i, bar in enumerate(bars_auc):\n",
    "    yval = bar.get_height()\n",
    "    error_top = auc_errors[1, i]  # Get the upper error bar for this bar\n",
    "    axes[1].text(bar.get_x() + bar.get_width() / 2, yval + error_top - 0.06,  # Slight offset above error bar\n",
    "                 f'{yval:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print AUC error bars on screen\n",
    "print(\"AUC Confidence Intervals:\")\n",
    "for i, name in enumerate(names):\n",
    "    mean = auc_means[i]\n",
    "    lower = auc_ci_lowers[i]\n",
    "    upper = auc_ci_uppers[i]\n",
    "    sd = auc_stds[i] if 'auc_stds' in locals() else 'N/A'\n",
    "    n = auc_ns[i] if 'auc_ns' in locals() else 'N/A'\n",
    "    print(f\"{name}: mean = {mean:.3f}, SD = {sd if isinstance(sd, str) else f'{sd:.3f}'}, N = {n}\")\n",
    "    print(f\"    CI = [{lower:.3f}, {upper:.3f}]\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188412a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy and AUC with Confidence Intervals\n",
    "\n",
    "print('Test Wave:',info_dict['test_wave'])\n",
    "print('Sex:',info_dict['sex_gender']) # None is both\n",
    "print('Include environmental?',info_dict['include_environmental_data'])\n",
    "print('Include genetic?',info_dict[\"include_genetic_data\"])\n",
    "print(\"Model Performance Metrics:\")\n",
    "for i, name in enumerate(names):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy_means[i]:.3f} \"\n",
    "          f\"(95% CI: {accuracy_ci_lowers[i]:.3f} - {accuracy_ci_uppers[i]:.3f})\")\n",
    "    print(f\"  AUC:      {auc_means[i]:.3f} \"\n",
    "          f\"(95% CI: {auc_ci_lowers[i]:.3f} - {auc_ci_uppers[i]:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0091fa",
   "metadata": {},
   "source": [
    "### The order is Logistic Regression [0], Decision Tree [1], SVM (Linear) [2], SVM (RBF) [3], XGBoost [4]  and Neural Network [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ac530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise t-test\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "import numpy as np\n",
    "\n",
    "# Create dictionary using enumerate to map names to their indices\n",
    "model_indices = {name: index for index, name in enumerate(names)}\n",
    "\n",
    "def compare_distributions(auc_scores_1, auc_scores_2, dist_name_1=\"Distribution 1\", dist_name_2=\"Distribution 2\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compares two distributions and returns whether they are statistically different.\n",
    "    \n",
    "    Returns:\n",
    "    (dist_name_1, dist_name_2, p_ttest, p_wilcoxon)\n",
    "    \"\"\"\n",
    "    if len(auc_scores_1) != len(auc_scores_2):\n",
    "        raise ValueError(\"The arrays must have the same length for paired tests.\")\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(auc_scores_1, auc_scores_2,equal_var=False)\n",
    "    #w_stat, p_wilcoxon = wilcoxon(auc_scores_1, auc_scores_2)\n",
    "    \n",
    "    return dist_name_1, dist_name_2, t_stat, p_value#, p_wilcoxon\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for name1, name2 in combinations(names, 2):\n",
    "    aucs_1 = aucs[model_indices[name1]]\n",
    "    aucs_2 = aucs[model_indices[name2]]\n",
    "\n",
    "    dist1, dist2, t_stat, p_value = compare_distributions(aucs_1, aucs_2, dist_name_1=name1, dist_name_2=name2, alpha=alpha)\n",
    "\n",
    "    #if p_ttest >= alpha and p_wilcoxon >= alpha:\n",
    "    #    print(f\"No significant difference between {dist1} and {dist2}:\")\n",
    "    #    print(f\"  Paired t-test p = {p_ttest:.5f}, Wilcoxon p = {p_wilcoxon:.5f}\")\n",
    "    print(f\"{dist1} vs. {dist2} (t = {abs(t_stat):.2f}, p = {p_value:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rossmann does not allow to download from the MLstatkit\n",
    "\"\"\"\n",
    "from MLstatkit import Delong_test \n",
    "\n",
    "#models = [\n",
    "    \"Logistic Regression\", \"Decision Tree\", \"SVM (Linear)\", \n",
    "    \"SVM (RBF)\", \"XGBoost\", \"Neural Network\"\n",
    "]\n",
    "\n",
    "# Iterate over all pairs of models (i.e., rows 0-1, 0-2, ..., 4-5)\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):  # Avoid comparing a model to itself\n",
    "        # Get the rows from the DataFrame for the models i and j\n",
    "        model_1 = df.iloc[i].tolist()\n",
    "        model_2 = df.iloc[j].tolist()\n",
    "\n",
    "        # Perform the DeLong test\n",
    "        z, p = Delong_test(y_test, model_1, model_2, return_ci=False, return_auc=False, verbose=0)\n",
    "\n",
    "        # Print the results for this pair of models\n",
    "        print(f\"Comparison: {models[i]} vs {models[j]}\")\n",
    "        print(f\"z = {z:.6f}, p = {p:.3e}\")\n",
    "        print(\"-\" * 50)  # For separation between comparisons\n",
    "        \n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd533fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anova\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Create dictionary using enumerate to map names to their indices\n",
    "model_indices = {name: index for index, name in enumerate(names)}\n",
    "\n",
    "def compare_distributions_anova(auc_scores_list, model_names, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs one-way ANOVA to compare multiple distributions and checks if there's a significant difference.\n",
    "    \n",
    "    Returns:\n",
    "    (anova_statistic, p_value)\n",
    "    \"\"\"\n",
    "    # Perform ANOVA test\n",
    "    f_stat, p_value = stats.f_oneway(*auc_scores_list)\n",
    "    \n",
    "    return f_stat, p_value\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# Prepare list of AUC scores for all models\n",
    "auc_scores_list = [aucs[model_indices[name]] for name in names]\n",
    "\n",
    "# Perform ANOVA across all distributions\n",
    "f_stat, p_value = compare_distributions_anova(auc_scores_list, names, alpha)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"Significant difference found between distributions (F-statistic = {f_stat:.2f}, p = {p_value:.3f})\")\n",
    "else:\n",
    "    print(f\"No significant difference between distributions (F-statistic = {f_stat:.2f}, p = {p_value:.3f})\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "namee = 'XGBoost'\n",
    "print(namee)\n",
    "print('mean =',round(aucs[model_indices[namee]].mean(),4))\n",
    "print('std =',round(aucs[model_indices[namee]].std(),4))\n",
    "print('SE =',round(stats.sem(aucs[model_indices[namee]]),4))\n",
    "print('N =',len(aucs[model_indices[namee]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reversing the dictionary to map index to model name\n",
    "index_to_model = {v: k for k, v in model_indices.items()}\n",
    "\n",
    "def get_model_by_index(index):\n",
    "    return index_to_model.get(index, \"Model not found\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "tab10 = plt.get_cmap('tab10')\n",
    "color_pallet = {i: tab10(i) for i in range(6)}\n",
    "\n",
    "\n",
    "all_counts = []\n",
    "all_means = []\n",
    "\n",
    "for i in range(len(model_indices)):\n",
    "    data = aucs[i]\n",
    "\n",
    "    counts, bins, patches = plt.hist(\n",
    "        data,\n",
    "        bins=20,\n",
    "        histtype='step',\n",
    "        linewidth=2,\n",
    "        edgecolor=color_pallet[i],\n",
    "        label=get_model_by_index(i)\n",
    "    )\n",
    "    all_counts.append(max(counts))\n",
    "\n",
    "    mean_auc = np.mean(data)\n",
    "    all_means.append(mean_auc)\n",
    "    plt.axvline(\n",
    "        mean_auc,\n",
    "        color=color_pallet[i],\n",
    "        linestyle='--',\n",
    "        linewidth=1\n",
    "    )\n",
    "\n",
    "max_y = max(all_counts)\n",
    "arrow_y_base = max_y * 0.05\n",
    "arrow_y_spacing = max_y * 0.08\n",
    "\n",
    "text_y_base = -max_y * 0.05  # Base height for text below x-axis\n",
    "text_y_step = -max_y * 0.03  # Step to vary height and reduce overlap\n",
    "\n",
    "for i in range(len(model_indices)):\n",
    "    data = aucs[i]\n",
    "    ci_low, ci_high = np.percentile(data, [2.5, 97.5])\n",
    "    mean_auc = all_means[i]\n",
    "    color = color_pallet[i]\n",
    "\n",
    "    # Draw CI arrows\n",
    "    arrow_y = arrow_y_base + i * arrow_y_spacing\n",
    "    plt.annotate(\n",
    "        '', xy=(ci_high, arrow_y), xytext=(ci_low, arrow_y),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle='<->',\n",
    "            color=color,\n",
    "            linewidth=1.\n",
    "        )\n",
    "    )\n",
    "\n",
    "    text_y = text_y_base + i * text_y_step  # Vary y to reduce stacking\n",
    "\n",
    "    text = f\"{mean_auc:.3f} [{ci_low:.3f}–{ci_high:.3f}]\"\n",
    "    plt.text(\n",
    "        mean_auc, text_y,\n",
    "        text,\n",
    "        color=color,\n",
    "        fontsize=9,\n",
    "        ha='center',\n",
    "        va='top',\n",
    "        rotation=0\n",
    "    )\n",
    "\n",
    "plt.xlabel('AUC Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrapping Distributions')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.25)  # More space for labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21097ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the DataFrame\n",
    "models_dict = {\n",
    "    \"Model\":models.values,\n",
    "    \"Name\": names,\n",
    "    \"AUC\": aucs,\n",
    "    \"AUC Mean\": auc_means,\n",
    "    \"AUC CI Lower\": auc_ci_lowers,\n",
    "    \"AUC CI Upper\": auc_ci_uppers,\n",
    "    \"Accuracy Mean\": accuracy_means,\n",
    "    \"Accuracy CI Lower\": accuracy_ci_lowers,\n",
    "    \"Accuracy CI Upper\": accuracy_ci_uppers,\n",
    "    \"Prob y\": y_pred_list\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "models_df = pd.DataFrame(models_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df['AUC'].iloc[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d185b",
   "metadata": {},
   "source": [
    "# Rank Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5319da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create a function to extract feature importance for each model\n",
    "def get_feature_importance(model, X_train, y_train, model_name):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Directly get feature importances (e.g., DecisionTree, XGBoost)\n",
    "        return model.feature_importances_\n",
    "    \n",
    "    elif isinstance(model, SVC) and model.kernel == 'linear':\n",
    "        # For SVM (Linear), use the absolute value of the coefficients\n",
    "        return np.abs(model.coef_[0])\n",
    "\n",
    "    elif isinstance(model, SVC) and model.kernel == 'rbf':\n",
    "        # Perform permutation importance and return only importances_mean\n",
    "        result = permutation_importance(model, X_train, y_train, n_jobs=-1, max_samples=0.02, n_repeats=2)\n",
    "        return result.importances_mean  # Extract the importance means\n",
    "\n",
    "    elif isinstance(model, LogisticRegression):\n",
    "        # For Logistic Regression, use the absolute value of the coefficients\n",
    "        return np.abs(model.coef_[0])\n",
    "\n",
    "    elif model_name == 'Neural Network':\n",
    "        # For Neural Network, use SHAP values\n",
    "        return np.abs(shap_values.values).mean(axis=0)  # Mean of absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cc565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Function to plot feature importances of multiple models using subplots\n",
    "def plot_feature_importances(models, X_train, y_train, top_n=10):\n",
    "    importances = {}\n",
    "\n",
    "    # Check if models are correctly passed\n",
    "    if not models:\n",
    "        print(\"No models provided.\")\n",
    "        return\n",
    "\n",
    "    # Create a subplot grid with an optimal number of rows and columns\n",
    "    model_names = list(models.keys())\n",
    "    num_models = len(model_names)\n",
    "\n",
    "    # Calculate the number of rows and columns based on the number of models\n",
    "    num_rows = math.ceil(np.sqrt(num_models))  # Approximate a square grid\n",
    "    num_cols = math.ceil(num_models / num_rows)\n",
    "\n",
    "    # Create subplots with calculated rows and columns\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5 * num_cols, 5 * num_rows))\n",
    "\n",
    "    # Flatten the axes array for easy indexing (in case it's a multi-dimensional grid)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through each model and plot its feature importances\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        # Get the feature importances\n",
    "        feature_importance = get_feature_importance(model, X_train, y_train, model_name)\n",
    "\n",
    "        # Rank the features based on importance (most to least important)\n",
    "        ranked_indices = np.argsort(feature_importance)[::-1]\n",
    "        sorted_importance = feature_importance[ranked_indices]\n",
    "        sorted_features = np.array(X_train.columns)[ranked_indices]\n",
    "\n",
    "        # Only take the top_n features\n",
    "        top_features = sorted_features[:top_n]\n",
    "        top_importance = sorted_importance[:top_n]\n",
    "\n",
    "        # Plot the top feature importances\n",
    "        axes[i].barh(top_features, top_importance, color='lightblue')\n",
    "        axes[i].set_title(f'{model_name} Feature Importances')\n",
    "        axes[i].set_xlabel('Importance')\n",
    "        axes[i].set_ylabel('Feature')\n",
    "\n",
    "        # Adjust the tick label sizes\n",
    "        axes[i].tick_params(axis='y', labelsize=10)\n",
    "        axes[i].tick_params(axis='x', labelsize=8)\n",
    "\n",
    "    # Hide any unused axes (if the grid size exceeds the number of models)\n",
    "    for j in range(num_models, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_feature_importances(models, X_train, y_train, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b3bc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Function to plot pairwise rank concordance\n",
    "def plot_pairwise_rank_concordance(models, X_train, y_train, top_n=10):\n",
    "    importances = {}\n",
    "    rankings = {}\n",
    "\n",
    "    # Check if models are correctly passed\n",
    "    if not models:\n",
    "        print(\"No models provided.\")\n",
    "        return\n",
    "\n",
    "    # Get the list of model names\n",
    "    model_names = list(models.keys())\n",
    "    print(\"Model names:\", model_names)  # Debugging line to check the list of model names\n",
    "\n",
    "    # Check if there are enough models\n",
    "    if len(model_names) < 2:\n",
    "        print(\"Not enough models for pairwise comparison.\")\n",
    "        return\n",
    "\n",
    "    # Store feature importances and rankings for each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "\n",
    "        # Get the feature importances\n",
    "        feature_importance = get_feature_importance(model, X_train, y_train, model_name)\n",
    "\n",
    "        # Rank the features based on importance (most to least important)\n",
    "        ranked_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "        # Save feature importances and their rankings\n",
    "        importances[model_name] = feature_importance\n",
    "        rankings[model_name] = ranked_indices\n",
    "    \n",
    "    # Calculate the number of subplots (pairwise comparisons)\n",
    "    num_comparisons = len(model_names) * (len(model_names) - 1) // 2\n",
    "    #cols = int(np.ceil(np.sqrt(num_comparisons)))  # Make grid as square as possible\n",
    "    #rows = int(np.ceil(num_comparisons / cols))\n",
    "    \n",
    "    # Set the number of columns to 3\n",
    "    cols = 3\n",
    "\n",
    "    # Calculate the number of rows needed for the comparisons\n",
    "    rows = int(np.ceil(num_comparisons / cols))\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 6))\n",
    "    axes = axes.flatten()  # Flatten axes for easy indexing\n",
    "\n",
    "    # Now compare the rank concordance (Spearman's rank correlation) between models\n",
    "    plot_index = 0\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i + 1, len(model_names)):\n",
    "            model1 = model_names[i]\n",
    "            model2 = model_names[j]\n",
    "\n",
    "            # Calculate Spearman's rank correlation between the rankings\n",
    "            rank_corr, _ = spearmanr(rankings[model1], rankings[model2])\n",
    "            print(f\"Spearman's rank correlation between {model1} and {model2}: {rank_corr:.4f}\")\n",
    "\n",
    "            ax = axes[plot_index]  # Get the next subplot axis\n",
    "\n",
    "            # Plot all features (light grey for the rest)\n",
    "            ax.scatter(importances[model1], importances[model2], c='lightgrey', edgecolors='k', alpha=0.7, s=300)\n",
    "\n",
    "            # Highlight the top `n` common features\n",
    "            common_top_features = list(set(rankings[model1][:top_n]) | set(rankings[model2][:top_n]))\n",
    "            for idx, k in enumerate(common_top_features):\n",
    "                ax.scatter(importances[model1][k], importances[model2][k], c=[plt.cm.tab20(idx)], s=300, label=X_train.columns[k], edgecolors='k', alpha=0.7)\n",
    "\n",
    "            # Add labels and title\n",
    "            ax.set_xlabel(f'{model1} Feature Importance')\n",
    "            ax.set_ylabel(f'{model2} Feature Importance')\n",
    "            ax.set_title(f\"Rank Concordance: {model1} vs {model2}\\nSpearman's Corr: {rank_corr:.4f}\")\n",
    "\n",
    "            # Legend and grid\n",
    "            ax.legend(title=\"Top Features\", fontsize=8, markerscale=0.5, handlelength=2)\n",
    "            ax.grid(True)\n",
    "\n",
    "            plot_index += 1\n",
    "\n",
    "    # Remove any unused axes\n",
    "    for i in range(plot_index, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot\n",
    "plot_pairwise_rank_concordance(models, X_train, y_train, top_n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e11cf",
   "metadata": {},
   "source": [
    "# Feature Importance Best model (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the model\n",
    "booster = models['XGBoost'].get_booster()\n",
    "\n",
    "# Get feature importance scores for each importance type\n",
    "importance_weight = booster.get_score(importance_type='weight')\n",
    "importance_gain = booster.get_score(importance_type='gain')\n",
    "importance_cover = booster.get_score(importance_type='cover')\n",
    "\n",
    "# Round the importance values to integers\n",
    "importance_weight = {k: round(v) for k, v in importance_weight.items()}\n",
    "importance_gain = {k: round(v) for k, v in importance_gain.items()}\n",
    "importance_cover = {k: round(v) for k, v in importance_cover.items()}\n",
    "\n",
    "# Sort the features by importance for each type and get the top 10\n",
    "top_10_weight = sorted(importance_weight.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_10_gain = sorted(importance_gain.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_10_cover = sorted(importance_cover.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Separate feature names and importance values for plotting\n",
    "features_weight, values_weight = zip(*top_10_weight)\n",
    "features_gain, values_gain = zip(*top_10_gain)\n",
    "features_cover, values_cover = zip(*top_10_cover)\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot feature importance based on 'weight' (F-score)\n",
    "axes[0].barh(features_weight, values_weight, align=\"center\")\n",
    "axes[0].set_title('Feature Importance - Weight (F-score)')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_ylabel('Features')\n",
    "\n",
    "# Plot feature importance based on 'gain'\n",
    "axes[1].barh(features_gain, values_gain, align=\"center\")\n",
    "axes[1].set_title('Feature Importance - Gain')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_ylabel('Features')\n",
    "\n",
    "# Plot feature importance based on 'cover'\n",
    "axes[2].barh(features_cover, values_cover, align=\"center\")\n",
    "axes[2].set_title('Feature Importance - Cover')\n",
    "axes[2].set_xlabel('Importance')\n",
    "axes[2].set_ylabel('Features')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5886c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(features_gain, values_gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd53cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature importance\n",
    "#plt.figure(figsize=(12, 8), dpi=800)\n",
    "\n",
    "\n",
    "# Plot using 'gain' as the importance type (which corresponds to Gini index)#\n",
    "#xgb.plot_importance(models['XGBoost'], importance_type='gain', max_num_features=10, height=0.8,show_values=True,values_format ='{v:.0f}')\n",
    "\n",
    "#plt.savefig(\"feature_importance_xgboost.png\", dpi=900)\n",
    "\n",
    "#plt.title(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to pandas Series\n",
    "# Convert dictionary to pandas Series\n",
    "series = pd.Series(y_test)\n",
    "\n",
    "# Save the Series to a CSV file as a single line of comma-separated values\n",
    "series.to_csv('y_test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the row where Name is 'XGBoost'\n",
    "xgboost_row = models_df[models_df['Name'] == 'XGBoost']\n",
    "\n",
    "# Extract the 'Prob y' value (this is still a single value in a series)\n",
    "prob_y_series = xgboost_row['Prob y']\n",
    "\n",
    "# Save the Series to a CSV file\n",
    "prob_y_series.to_csv('xgboost_prob_y.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
